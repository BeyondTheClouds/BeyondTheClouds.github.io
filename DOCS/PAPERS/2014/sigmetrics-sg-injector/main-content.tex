\section{Introduction}
\label{sec:intro}
%\AL[AL]{1 page (including the abstract)}

% Although a lot of progress has been made on Cloud Computing (CC)
% system management, \aka Infrastructure-as-a-Service
% toolkits~\cite{moreno:2012}, most of the popular
% solutions~\cite{cloudstack, opennebula, openstack} continue to rely on
%  elementary Virtual Machine (VM) placement
% policies that prevent them from maximizing the usage of CC resources
% while guaranteeing VM resource requirements as defined by Service
% Level Agreements (SLAs).
% %% THE TEXT BELOW COMES FROM ISPA'13
% %%%%%%%
% Typically, a batch scheduling approach is used: VMs (i)~are allocated
% according to user requests for resource reservations, and (ii)~are
% tied to the nodes where they were deployed until their
% destruction. Besides the fact that users often overestimate their
% resource requirements, such static policies are definitely not optimal
% for CC providers, since the effective resource requirements of each
% operated VM may significantly vary during its lifetime.
% %%%%%%%
%  Dynamic strategies such as consolidation, load balancing
% and other SLA-ensuring algorithms have been deeply investigated \cite
% {feller:ccgrid12}, \cite{Hermenier:2009:ECM:1508293.1508300},
% \cite{5715067}, \cite{quesnel:cpe2012}, \cite{5328077},
% \cite{5935254}. \MS{Be more assertive on the motivation: Adrien, which
%   was the article for justification?}

Even if more flexible and often more efficient approaches to the
Virtual Machine Placement Problem (VMPP) have been developed
%by the research community
, most of the popular Cloud Computing (CC) system
management \cite{cloudstack, opennebula, openstack}, \aka
Infrastructure-as-a-Service toolkits~\cite{moreno:2012}, continue to
rely on elementary Virtual Machine (VM) placement policies that
prevent them from maximizing the usage of CC resources while
guaranteeing VM resource requirements as defined by Service Level
Agreements (SLAs).
% %% THE TEXT BELOW COMES FROM ISPA'13
% %%%%%%%
Typically, a batch scheduling approach is used: VMs are allocated
according to user requests for resource reservations and tied to
the nodes where they were deployed until their destruction. Besides
the fact that users often overestimate their resource requirements,
such static policies are definitely not optimal for CC providers,
since the effective resource requirements of each operated VM may
significantly vary during its lifetime.

An important impediment to the adoption of more advanced strategies
such as consolidation, load balancing and other SLA-ensuring
algorithms that have been deeply investigated by the research community
\cite{feller:ccgrid12, Hermenier:2009:ECM:1508293.1508300, 5715067,
  quesnel:cpe2012, 5328077, 5935254} is related to the experimental
processes that have been used to validate them: most VMPP proposals have
been evaluated either by leveraging ad-hoc simulators or small
testbeds. These evaluation environments are not accurate and not
representative enough to (i) ensure their correctness on real
platforms and (ii) perform fair comparisons between them.

% %
% \begin{figure}[ht]
% \vspace*{-.2cm}
% \begin{center}
%         \subcapcentertrue
%         \subfigure[Scheduling steps]{
%         \includegraphics[width=.45\linewidth]{figures/scheduling_steps.pdf}
%         \label{fig:scheduling_steps}}
%         \subfigure[Workload fluctuations during scheduling]{
%         \includegraphics[width=.45\linewidth]{figures/workload_fluctuations2.pdf}
%         \label{fig:workload_fluctuations}}
% \vspace*{-.2cm}
% \caption{VM scheduling in a master/worker architecture}
% \end{center}
% \label{fig:scheduling}
% \vspace*{-.2cm}
% \end{figure}
% %
% \MS[AL]{Must we keep the fig.: the arguments of complexity and time
%   requirements are already made in the remainder of the intro. If we
%   keep it, the discussion should be simplified.}
% Each VMPP mechanism is a complex system that can face
% important side-effects during each of its stages: Monitoring the
% resources usages, computing the schedule and applying the
% reconfiguration (see Figure \ref{fig:scheduling_steps}).
% %
% %
% As an example, a single master architecture can lead, \textit{a priori} to
% important drawbacks. First, during the computation and the application
% of a schedule, a single master cannot take into account new VM
% requirement violations. Second, the time needed to apply a new
% schedule can be particularly important: The longer the reconfiguration
% process, the higher the risk that the schedule may be outdated, due to
% the workload fluctuations, when it is eventually applied (see Figure
% \ref{fig:workload_fluctuations}). Finally, a single master node can
% lead to well-known fault-tolerance issues: A group of VMs may be
% temporarily isolated from the master node in case of a network
% disconnection or if the master node crashes.
% %
Implementing each proposal and evaluating it on representative
testbeds in terms of scalability, reliability and varying workload
changes would definitely be the most rigorous way to observe and
propose appropriate solutions for CC production infrastructures.
% and compare  with existing proposals.
However, \textit{in-vivo} (\ie real-world) experiments, if they can be
executed at all, are always expensive and tedious to perform as it has
recently been highlighted once again~\cite{barker:pitfalls}. They may
even be counterproductive if the observed behaviors are far from the
expected ones due to parameters that influence experiments in complex
real-world contexts in unknown ways. \AL[MS]{This sentence is still unclear}

In this article, we propose \vmps, a dedicated simulation framework to
perform in-depth investigations of VM placement algorithms and compare
them in a fair way. To cope with real conditions such as the
increasing scale of modern data centers and the dynamicity of the
workloads that are specific to the CC paradigm, notably its elasticity
capacity, \vmps allows users to study large-scale scenarios that take
into account server crashes and that involve tens of thousands of VMs,
each of which executes
\AL[MS]{excutes or executing a} a specific workload that evolves during the
simulation lifetime.

Built on top of the \sg toolkit~\cite{casanova:hal-01017319}, \vmps
provides three additional simulation facilities: a more abstract
representation of hosts and virtual machines, and two frameworks, one
for the management of load injection abstract and another one for the
extraction and analysis of execution traces. We believe that such a
tool will be beneficial to a large number of researchers in the field
of CC as it enables them to quickly validate
% AL->MS the characterisctics -> the trends,  simulations are
% well-known scientific intrusments to validate trends (and only trends).
the trends of a new proposal and compare it with existing
ones. This way, our approach allows \textit{in vivo} experiments to be
restricted to VMPP mechanisms that have the potential to handle CC
production infrastructures.

%
We chose to base \vmps on \sg since (i) the latter's relevance in
terms of performance and validity has already been
demonstrated~\cite{simgridpub} and (ii) because it has been recently
extended to integrate virtual machine abstractions and a live
migration model \cite{Hirofuchi:2013:ALM:2568486.2568524}.

To illustrate the relevance of \vmps, we have implemented the
essential mechanisms of three well-known VMPP approaches:
Entropy~\cite{Hermenier:2009:ECM:1508293.1508300},
Snooze~\cite{feller:ccgrid12}, and DVMS~\cite{quesnel:cpe2012}. We
have also investigated their characteristics by analyzing their
scalability, reliability and reactivity (\ie the time to solve resource
violations, \aka SLA violation). Besides being well-known from the literature, we chose
these three systems as they are built on three different software
architecture models: Entropy uses a centralized model, Snooze a
hierarchical one and DVMS a fully distributed one.

The experiments reveal ...
\AL[AL]{What the experiments reveal at the end ? }

The rest of the article is
organized as follow. Section~\ref{sec:vmpp} highlights the importance
of the scalability, reliability and reactivity criterions for the VM
Placement Problem.
Section~\ref{sec:sg} gives an overview of the \sg
framework on which our proposal is built. \ref{sec:injector}
introduces \vmps and discusses its general functioning. The three
algorithms implemented as use-cases are presented in
Section~\ref{sec:vm-schedulers} and evaluated in
Section~\ref{sec:experiments}. Section~\ref{sec:related} and
Section~\ref{sec:conclusion} present, respectively, related work as
well as a conclusion and future work.

\section{VM Placement Problem Overview}
\label{sec:vmpp}

At coarse-frained, VMPP can be summarized in
three-steps (see Figure \ref{fig:scheduling_steps}):  Monitoring the
resources usages, computing a new schedule each time is it  needed and applying the
reconfiguration plan (\ie performing VM migration and supend/resume
operations to switch to the new placement solution).
% that are mandatory to solve resource violations while optimizing resource usages).

\begin{figure}[ht]
\vspace*{-.2cm}
\begin{center}
        \subcapcentertrue
        \subfigure[Scheduling steps]{
        \includegraphics[width=.45\linewidth]{figures/scheduling_steps.pdf}
        \label{fig:scheduling_steps}}
        \subfigure[Workload fluctuations during scheduling]{
        \includegraphics[width=.45\linewidth]{figures/workload_fluctuations2.pdf}
        \label{fig:workload_fluctuations}}
\vspace*{-.2cm}
\caption{VM scheduling Phases}
\end{center}
\label{fig:scheduling}
\vspace*{-.2cm}
\end{figure}
%

As the goal  of VMPP solutions is to maintain a placement that
satisifies the requirements of each VM while optimizing CC resource usages, scalability,
reliability and reactivity of the mechanisms implementing each of
these three steps are decisive. For instance, a naive implementation
of a master/workers approach as described in
Figure~\ref{fig:scheduling_steps} would prevent to take into account
workload fluctuations during the computation and the application of a
schedule, leading potentially to artifical violations (\ie resource violations
that are due to the VMPP mechanism). In other words, the longer each
phase,
% the longer the reconfiguration process,
the higher the risk that the schedule may be outdated when it is
computed or eventually applied (see Figure
\ref{fig:workload_fluctuations}). Similarly, servers and network
crashes can prevent to detect and solve resource violations if the
master node crashes or if a group of VM is temporarily isolated from
the master node.

Analyzing how VMPP solutions behave when such aforementioned events
occur is mandatory to validate them. Providing a framework to make
such studies easier and in a more reproducible manner is the target of \vmps as described in the
following.

\section{Simgrid, a generic toolkit}
\label{sec:sg}
%\AL[AL]{0.5page}

\sg is a simulation toolkit to study the behavior of
large-scale distributed systems.  Developped for more than  a decade, it has been used in more than 100
publications.  Its main characterisitics are related to its:
\begin{itemize}
\item Extensibility -- After Grids, HPC and P2P
  systems, \sg has been recently extended with virtualization technologies abstractions
(\ie Virtual Machines including a live migration model \cite{Hirofuchi:2013:ALM:2568486.2568524}) to allow users to investigate Cloud
Computing challenges \cite{lucas:cloud2014};
\item Scalability -- It is possible to simulate large-scale scenarios,
  as an example, users can simulate applications composed of 2
  Millions of processors and an infrastructure composed of 10,000~servers hosting more than
  100,000~VMs on a computer with 16~GB of memory;
\item  Flexibility -- It allows to run a simulation on arbitrary network
  topology under dynamic computation and network resources
  availabilities;
\item API --  users can leverage \sg through easy-to-use APIs in C
  and Java.
\end{itemize}

To perform simulations, users should develop a \emph{program} and
define a \emph{platform} file and a \emph{deployment} file. The
\emph{program} in most cases leverages the \sg MSG API that allows
end-users to create and execute \sg abstractions such as processes,
tasks, VMs and network-communications. The \emph{platform} file gives
the physical description of each resource composing the environment
and on which aforementioned computations/network exchanges will be
performed in the \sg
world.
% (host, CPU capacity, network topology and link capacities, etc.)
Finally, the \emph{deployment} file is used to launch the different
\sg processes defined in the \emph{program} on the different nodes (at
least the mapping between one process and one host is mandatory to
start the simulation). The execution of the program is orchestrated by
the \sg engine that internally relies on an constraint solver to
correctly assign the amount of CPU/network resources to each \sg
abstraction, step by step until the simulation ends.

Although SimGrid has many features such as model checking, the
simulation of DAGs (Direct Acyclic Graphs) or MPI applications, we
only give a brief description of the  virtualization  abstractions
that have been recently implemented in \sg and on which our framework relies
on.  Further information regarding \sg is available in \cite{casanova:hal-01017319}.

The VM support has been designed so that all operations that can be performed
on a host can be performed inside a VM: From the point of view of a \sg
Host, a \sg VM is considered as an ordinary task while from the point
of view of a task running inside a \sg VM, a VM is considered as an
ordinary host below the task.  By such a mean, \sg users can easily
switch between a virtualized and non virtualized infrastructure.
Moreover, thanks to  MSG API extensions, users can control VMs in the
same manner as in the real world (\eg create/destroy, start/shutdown,
suspend/resume and migrate).
% TODO: Not addressed
%\AL{Shoudl we talk about over-provisionning limitations}
For migration operations, a VM live migration model implementing the
precopy migration algorithm of Qemu/KVM has been integrated into \sg.
This model is the only one that successfully simulates the live
migration behavior by taking into account the resource sharing
competition as well as the memory refreshing rate of the VM, thus
determining correctly the live migration time as well as the resulting
network traffic~\cite{Hirofuchi:2013:ALM:2568486.2568524}.

%\AL[AL]{Add more details regarding live migration in order to reply to
%the Mario's remark (not clear enoug)}

Those two capabilities were mandatory to build our VM placement
simulator toolkit introduced in the next section.

\section{VM Placement Simulator}
\label{sec:injector}
%\AL[AL]{1.5 page}

The purpose of \vmps is to deliver a generic tool to evaluate new VM
placement algorithms and offer the possibility to compare
them. Concretely, it supports the management of VM creations, workload
fluctuations as well as node apparitions/removals.  Researchers can
thus focus on the implementation of new placement algorithms and
evaluate how it behaves according to the different changes that occur
during the simulation.
%
\vmps has been implemented in Java by leveraging the messaging API
(MSG) of \sg.  Although the Java layer has an impact of the efficiency
of \sg, we believe its use is acceptable because Java offers important
benefits to researchers for the implementation of advanced scheduling
strategies, notably concerning the ease of implementation of new
strategies.
% TODO: not adressed
%\AL[JP]{Should we talk about SCALA?}

In the following we give an overview of the framework and describe its
general functioning.% and how researchers can develop new algorithms

\subsection{Overview}
\label{sec:overview}

From a high-level view, \vmps performs a simulation in three phases:
(i) initialization (ii) injection and (iii) trace analysis (see Figure
\ref{fig:workflow}).  The
initialization phase corresponds to the creation of the environment,
the VMs and the generation of the event queue. During the injection
phase the scheduling strategy is evaluated. Basically, it consists of
at least two \sg processes, one executing the \emph{injector} and a
second one executing the to-be-simulated scheduling algorithm.  The
\emph{injector} constitutes the generic part of the framework. It
injects scheduling-relevant events during the execution of
simulations.  Currently, the supported events are VM CPU load changes
and node apparitions/removals that we use to simulate node crashes.

\begin{figure}
  {\centering ~\includegraphics[width=.95\linewidth]{figures/VMPlaceS-workflow.png}}
  \caption{\vmps's Workflow}
  \label{fig:workflow}
{\small Gray parts correspond to the generic code while the white one
  must be provided by end-users. The current version is released with
  three different schedulers (centralized/hierarchical and distributed).}
\end{figure}

% As we describe in the next section, additional events can be easily added.
%\AL[AL]{Make two figures: a architectural one (i.e. Injector vs
 % schedulers pool and one chronological.}
%\MS{Yes, the figures are important. They could also be useful to
 % partially provide a more abstract explanation.}

Regarding the scheduling algorithm, users should develop their own
algorithm by leveraging the \sg messaging API and the \texttt{XHost},
\texttt{XVM} and \texttt{SimulatorManager} classes. The two former
classes respectively extend \sg's \texttt{Host} and \texttt{VM}
abstractions while the latter controls the interactions between the
different components of the VM placement simulator.  Throughout these
three classes, users can inspect, at any time, the current state of
the infrastructure (\ie the load of a host/VM, the number of VMs
hosted on the whole infrastructure or on a particular host, check
whether one host is overloaded, etc.) We have used the \vmps in order
to analyze three scheduling mechanisms, cf.\
Sec.~\ref{sec:vm-schedulers}. We choose these three mechanisms as they
represent three different software architecture models: centralized,
hierarchical and fully-distributed models for VM placement.
%% TODO
%\MS{The
%  following point is too low-level and should not come here} Although
%we do not discuss that point due to space constraints, we emphasize
%that these three mechanisms enable us to deliver concrete examples of
%how the deployment file of \sg is automatically generated by leveraging
%a generic python script.  \AL{We should highlight that point in the
%  README.org}
The last phase consists in analyzing the collected traces in order to
gather the results of the simulation, notably by means of the
generation of figures representing, \eg resource usage statistics.

%\begin{itemize}
%\item Entropy \cite{Hermenier:2009:ECM:1508293.1508300}, a centralized approach using a constraint programming approach to solve the placement/reconfiguration VM problem;
% \item Snooze \cite{feller:ccgrid12}, a hierarchical approach where
%   each manager of a group invokes Entropy to solve the
%  placement/reconfiguration VM problem. It is noteworthy that in
%   \cite{feller:ccgrid12}, Snooze is using a specific heuristic to solve the placement/reconfiguration VM problem. As the sake of simplicity, we have simply reused the entropy scheduling code.
%\item  DVMS \cite{quesnel:cpe2012}, a distributed approach that dynamically partitions the system and invokes Entropy on each partition.
% \end{itemize}

\subsection{Initialization Phase}

In the beginning, \vmps creates $n$ VMs and assigns them in a
round-robin manner to the first $p$ hosts defined in the platform
file.  The default platform file corresponds to a cluster of $p+s$
hosts, where $p$ corresponds to the number of hosting nodes and $s$ to
the number of services nodes. The values $n$, $p$ and $s$ constitute
input parameters of the simulations (specified in a Java properties
file).
%% TODO
% \AL[AL]{Update the size of the cluster autonomically by
%  leveraging p + s}
Although the cluster topology is the most
common one, it is possible to define more complex platforms to simulate,
for instance, scenarios involving federated data centers.
%Note that $s$ can be equals to 0 if the
%scheduling strategy is directly executed on the hosting nodes.

Each VM is created based on one of the predefined VM classes. A VM
class corresponds to a template specifying the VM attributes and its
memory footprint. Concretely, it is
% described as
% \texttt{nb\_cpu:ramsize:net\_bw:mig\_speed:mem\_speed}
defined in terms of five parameters:
\texttt{nb\_cpus}  the number of cores, \texttt{ramsize }the size of
the memory, \texttt{net\_bw} the
network bandwidth, \texttt{mig\_speed} the maximum
bandwidth available migrate it and \texttt{mem\_speed} the maximum
memory update speed when the VM is consuming 100\% of its CPU
resources. As pointed out in Section \ref{sec:sg}, the memory update speed is a critical parameter that
governs the migration time as well as the amount of transferred
data. By giving the possibility to define VM classes,
\vmps allows researchers to simulate different kinds of workload (\ie
memory-intensive vs non-intensive workloads), and thus analyze more
realistic Cloud Computing problems.
Available classes are
defined in a specific text file that can be modified according to the
user's needs.
%\MS{Follows a low-level mechanism!}
% TODO not addressed yet. This text should appear in the README
%At creation time
%of a VM, a process selects one class (\ie one line) in the file
%randomly. Hence, if a user wants to favor a specific class, he can
%simply repeat the line of the class several times.
%
Finally, all VMs start with a CPU consumption of 0 that will evolve during the
simulation after charge injection as explained below.

Once the creation and the assignment of VMs is completed, \vmps spawns
at least two SG processes, the \emph{injector} and the launcher of the
selected scheduler.  The first action of the \emph{injector} consists
in creating the different event queues and merge them into a global
one that will be consumed during the second phase of the simulation.
For now, we generate two kinds of event: ~~\emph{CPU load} and
\emph{node crash} events.
%\todo{Before apparitions/ removals}
% The former consists in changing the load of a VM by creating and
% assigning a new \sg task in the VM while the second aims at
% simulating crashes.
%
% Changing the load of a VM has a direct impact of its memory update
% speed and thus on the time to migrate it between two hosts.
The \emph{CPU load} event queue is generated in order to change the
load of each VM every $t$ seconds on average. $t$ is a random variable
that follows an exponential distribution with rate parameter
$\lambda_t$ while the CPU load of a VM evolves according to a
Gaussian distribution defined by a particular mean ($\mu$) as well as
a particular standard deviation ($\sigma$). $t$, $\mu$ and $\sigma$
are provided as input parameters of a simulation.  As the CPU load can
fluctuate between 0 and 100\%, \vmps prevents the assignment of
nonsensical values when the Gaussian distribution returns a number
smaller than 0 or greater than 100. Although this has no impact on the
execution of the simulation, we highlight that this can
reduce/increase the effective mean of the VM load, especially when
$\sigma$ is high.  Hence, it is important for users to specify
appropriate values.
%% TODO
%\AL[AL]{A binomial law would have solved this issue: too late too bad :(}
%Although this can have an impact on the
%effective mean, especially when $\sigma$ is high, we believe it was
%non appropriated to request it is easier for end-users to specify $\mu$ and
%$\sigma$ parameters than
We highlight that each
random process used in \vmps is initialized with a seed that is
defined in a configuration file. This way, we can ensure that
different simulations are reproducible and may be used to establish
fair comparisons.

The \emph{node crash} event queue is generated in order to turn off a
node every $f$ seconds on average for a duration of $d$ seconds.
Similarly to the $t$ value above, $f$ follows an exponential
distribution with rate $\lambda_f$. $f$ and $d$ are also provided as
input parameters of a simulation.

Adding new events can easily be done by simply defining new event Java
classes implementing the \texttt{InjectorEvent} interface and by
adding the code in charge of generating the associated queue. Such a
new queue will be merged into the global one and its events will then be
consumed similarly to other ones during the \emph{injector Phase} as described in the following.

\subsection{Injector Phase}

Once the VMs and the global event queue are ready, the evaluation of
the scheduling mechanism can start. First, the injector process
iteratively consumes the different events that represent, for now,
load changes of a VM or turning a node off or on. Changing the load of
a VM corresponds to the creation and the assignment of a new \sg task
in the VM. This new task has a direct impact on the time that will be
needed to migrate the VM as it increases or decreases the current CPU
load and thus the percentage of its memory update speed.
\AL[MS]{Is the above paragraph clear enough?}
% that is indicated by the \texttt{mem\_speed}
%% parameter given in the class description.
When a node is turning off, the VMs that were running on that node are
temporarily discarded, \ie they are hidden and cannot be accessed
until the node comes back to life. This way, the scheduler cannot
handle them.
 %\AL[AL, MS, JP]{This is ugly but unfortunately the true,
 % it will be better to reassign those VMs on other nodes, but which
 % one?  }
 We leave for future work other approaches that can better
match realistic scenarios such as turning off the VMs and
reprovisioning them on other nodes.
%

As defined by the scheduling algorithm VMs will be suspended/resumed
or relocated throughout the available hosts to meet scheduling
objectives and SLA guarantees.  Note that users must implement the
algorithm in charge of solving the VMPP but also the code in charge of
applying reconfiguration plans by invoking the appropriate methods
available from the \texttt{SimulatorManager} class. This step is
essential as the reconfiguration cost is a key element of dynamic
placement systems.  \AL{maybe it is better to prevent the access to
  Xhost and XVM methods that can change the Simulator States. Hence,
  we should enforce the access only through the SimulatorManager class
  ? What do you think ?}  Last but not the least, it is noteworthy
that \vmps really invokes the execution of each scheduling strategy in
order to get the effective reconfiguration plan.  That is, the
computation time that is observed is not simulated but corresponds to
the effective one, only the workload inside the VMs and the migration
operations are simulated in \sg. It is hence mandatory to propagate
the reconfiguration time into the \sg engine by invoking a
\texttt{wait} call of the MSG interface.

\subsection{Trace Analysis}
\label{subsec:traces-analysis}

The last step of \vmps consists in analyzing the information that has
been collected during the simulation.
% in order to understand and compare the behavior of the different
% algorithms.
This analysis is done in a two-step fashion. First, \vmps records
several metrics related to the platform utilization throughout the
simulation by leveraging an extended version of \sg's TRACE
module~\footnote{\url{http://simgrid.gforge.inria.fr/simgrid/3.12/doc/tracing.html}}.
This way, visualization tools promoted by the \sg community, such as
PajeNG~\cite{pageng:www} may be used. Furthermore, our extension
enables the creation of a trace file in the JSON file format, which is
used to generate several figures using the R statistical environment~\cite{R:Bloomfield:2014}.

By default \vmps records the load of the different VMs and hosts, the
appearance and the duration of each violation of VM requirements in
the system, the number of migrations, the number of times the
scheduler mechanism has been invoked and the number of times it
succeeds or fails to resolve non-viable configurations.
%
Although these pieces of information are key elements to understand
and compare the behavior of the different algorithms, we highlight
that the TRACE API enables the creation of as many variables as
necessary, thus allowing researchers to instrument their own algorithm
with specific variables that record other pieces of information.

\section{Dynamic VMPP Algorithms}
\label{sec:vm-schedulers}
To illustrate the interest of \vmps, we implemented three dynamic VM
placement mechanisms respectively based on the Entropy
\cite{Hermenier:2009:ECM:1508293.1508300}, Snooze
\cite{feller:ccgrid12}, and DVMS \cite{quesnel:cpe2012} proposals. For the three
implementations, we chose to use the latest VMPP solver that has been
developed as part of the Entropy
framework~\cite{hermenier:cp11}.

%
Giving up consolidation
optimality in favor of scalability, this algorithm provides a ``repair
mode'' that enables the correction of VM requirement violations. The algorithm considers that a host is
overloaded when the VMs try to consume more than 100\% of the CPU
capacity of the host. In such a case, the algorithm looks for
an optimal viable configuration until it reaches a predefined timeout.
The optimal solution is a new placement that satisfies
the requirements of all VMs while minimizing the cost of the
reconfiguration.
Once the timeout has been triggered, the algorithm returns
the best solution among the ones it finds and applies the associated
reconfiguration plan by invoking live migrations in the simulation
world.

%
Although using the Entropy VMPP solver
implies a modification from the original Snooze proposal,  we
highlight that our goal is to illustrate the capabilities of \vmps and
thus we believe that such a modification is acceptable as it does not
change the global behavior of Snooze. Moreover by
conducting such a comparison, we also investigate the pros and cons of
the three  architecture models on which these proposals rely on (\ie centralized, hierarchical and
distributed).

%
Before discussing the simulation results, we
describe in this section an overview of the three implemented systems.
We highlight that the extended abstractions for hosts (\texttt{XHost})
and VMs (\texttt{XVM}) as well as the available functions of the \sg
MSG API enabled us to develop them in a direct and natural manner.


\subsection{Entropy-based Centralized Approach}
\label{subsec:entropy}
The centralized VM placement mechanism consists in one single \sg
process deployed on a service node. This process implements a simple loop that
iteratively checks the viability of the current configuration by
invoking every $p$ seconds the aforementioned VMPP solver. $p$ is
defined as an input parameter of the simulation.

% \AL{Should we explain the issue right now or not if we add VMPP section}
% Indeed, during
% the computation and the application of a schedule, the algorithm does
% not enforce QoS properties anymore, and thus cannot react quickly to
% violations. Second, since the manipulation of VMs is costly, the time
% needed to apply a new schedule is particularly important: The longer
% the reconfiguration process is, the higher is the risk that the schedule may
% be outdated, due to the workload fluctuations, when it is eventually
% applied.
% \vmps enables researchers to investigate such concerns in-depth.

As the Entropy proposal does not provide a specific mechanism for the
collect of resource usage information but simply uses an external tool
(namely ganglia), we had two different ways to implement the monitoring to
process:  either by implementing additional asynchronous transmissions
as a real implementation of the necessary state updates would proceed
or, in a much more lightweight manner, through direct accesses by the
aforementioned process to the states of the hosts and their respective
VMs. While the latter does not mimic a real implementation closely, it
can be harnessed to yield a valid simulation: overheads induced by
communication in the ``real'' implementation, for instance, can be
easily added as part of the lightweight simulation. We have
implemented this lightweight variant for the monitoring

Regarding fault tolerance, similarly to the Entropy proposal, our
implementation does not provide any failover mechanism.

Finally, as mentioned in Section \ref{subsec:traces-analysis}, we monitor, for each iteration,
whether the VMPP solver succeeds or fails. In case of success, \vmps
records the number of migration that has been performed, the time it
took to apply the reconfiguration and whether
the application of the reconfiguration plan led to new violations.

\subsection{Snooze-based Hierarchical Approach}
\label{subsec:snooze}
\input{snooze.tex}

\subsection{DVMS-based Distributed Approach}
\label{subsec:dvms}
% TODO Not adressed
%\AL[AL]{Check who write that part, If Flavien did it, then add him as
%  an author}
\input{dvms}
\section{Experiments}
\label{sec:experiments}
\AL[JP,AL,MS]{2 pages}
\AL{Il faudra parler du nombre de migrations qui est egalement une
  m√©trique pertinente. Plusieurs algorithms tentent de reduire cette
  metrique }
\AL[AL]{Il faudra mettre des snapshots de PajeNG}
\subsection{Analysing and comparing different solutions}

\subsubsection{Reactivity and Violation time}

\subsubsection{Fault Tolerance}
\AL[JP]{perform an experiment on Snooze and DVMS with fault
  periodicity of 10 days for instance}
\MS[JP]{perform an experiment on Snooze with additional fautls (i.e.
  reducing the fault periodicity) in order to illustrate the pros/cons
of the two different join mechanisms. To switch between one and the
other strategies, see AUX.java line 34, GLElectionForEachNewGM = false/true}
\MS[JP]{}


\subsection{One step ahead thanks to \vmps}

\subsubsection{Snooze analysis}
\label{sec:snoozeVariants}

Our simulation framework facilitates the simulation of variants of
placement algorithms. In the following, we present three non-trivial
variants that we have implemented and explored: a variant of the
assignment algorithm of LCs to GMs, periodic vs.\ reactive scheduling,
and a variant of the algorithms of how GMs and LCs join the system.


\paragraph{Assignment of LCs to GMs}

LCs are assigned to GMs by the GL as part of the LC join protocol. In
Snooze's native implementation LCs are assigned in a round-robin
fashion to the known GMs. If GMs join (and leave) the system at the
same time as LCs, a round-robin strategy at join time, however, does
not ensure an even distribution. This may happen, for instance at
startup time of the system, when new GMs and LCs enter the system, or
in case of failures, which trigger GM and LC joins. In order to
evaluate the imbalance resulting from a round-robin strategy (as well
as others) we have implemented the LC assignment protocol in a modular
fashion and applied it in diverse highly-dynamic settings in which GMs
and LCs enter the system at the same time. Furthermore, we have
implemented a best-fit strategy that assigns LCs to GMs with minimal
load or to GMs with the smallest number of assigned LCs (if several
GMs with minimal load exist). The best-fit strategy can significantly
improve the scheduling characteristics of hierarchical placement
algorithms as shown by the experimental data presented in
Sec.~\ref{sec:snoozeVariantsEval}. Furthermore, it should always be
at least as good as the round-robin strategy (the corresponding proof
is left to future work).


\paragraph{Periodic vs.\ reactive scheduling}

Snooze~\cite{feller:ccgrid12} schedules VMs in a periodic fashion:
after a fixed time period a GM calls the scheduler in order to resolve
resource conflicts among the LCs it manages. The information whether a
resource conflict has to be handled is taken based on the summary
information that is periodically sent by the LCs to the GM.

We have provided an alternative, reactive, strategy to scheduling: as
soon as they occur, LCs avert their GMs of resource conflicts; the GMs
then initiate scheduling. Implementing this reactive scheme can be
done using our framework in two manners: either by implementing
additional asynchronous transmissions as a real implementation of the
necessary state updates would proceed or, in a much more lightweight
manner, through direct accesses by the GMs to the states of their
respective LCs. While the latter does not mimic a real implementation
closely, it can be harnessed to yield a valid simulation: delays
induced by communication in the ``real'' implementation, for instance,
can be easily added as part of the lightweight simulation. We have
implemented this lightweight variant of reactive scheduling.


\paragraph{Variants of the join algorithms}

The join algorithms, see Sec.~\ref{sec:snoozeAlgs}, are crucial to the
correctness of Snooze for two main reasons: (i) they have to be
efficient because they can easily form a bottleneck if large numbers
of LCs (GMs) have to be registered at a GM (LC); (ii) they are
multi-phase protocols whose correctness especially in the presence of
faults is difficult to ensure.

In order to investigate the corresponding trade-offs, we have used our
framework to implement join algorithms that may be interrupted at any
time, repeat the the on-going phase a number of times before
reinitiating, if necessary, the entire protocol. Furthermore, the join
protocol is parameterized, \eg, in the number of threads used to
handle registration requests.

Finally, our framework has enabled us to test another aspect of
Snooze's join algorithm as presented by
Feller~\etal.~\cite{feller:ccgrid12},
\MS[MS]{If we succeed to perform the experiment comparing both
  approach, this paragraph should be highlighted.}
a strategy we call the GM rejoin
strategy (GRJ): all GMs should rejoin if a new GM enters the
system. While GRJ supports a form of load balancing (because all LCs
are reassigned to the new set of GMs), our simulation has shown that
this strategy significantly increases the time necessary for
registering GMs and LCs compared to a simpler strategy that does not
modify existing GMs in case a new GM enters the system. This handicap
is particularly pronounced if joins of GMs may be interrupted due to
faults. Concretely, experiments involving 20 GMs and 200 LCs have
shown that this strategy often multiplies the time necessary to join
all 220 components by 10 or more compared to the simple join
strategy. While the qualitative result that the more complex strategy
presented in the paper results in a more time-consuming join process
is not very surprising, the extent of the resulting degradation was
surprising.



\subsubsection{LC assignment in Snooze-like placement alg.}
\label{sec:snoozeVariantsEval}

\begin{figure}[ht]
\begin{center}
    \includegraphics[width=.95\linewidth]{figures/violationTime-snooze-RR-BF.pdf}
    \caption{Cumulated violation time for BF (lower line) and RR
      (upper line) variants}
\end{center}
\label{fig:snoozeBFRRViolation}
\end{figure}

\AL[MS]{Put a table instead of the figure, it is shorter and you can
  add additional information such as gain, impact on the violation
  time etc.}

We have evaluated the two LC assignment algorithms for Snooze-like
placement algorithms, round-robin (henceforth, RR) and best-fit (BF)
that have been introduced in Sec.~\ref{sec:snoozeVariants}, using
\vmps on configurations of~128 and~256 nodes. In order to clearly
expose the corresponding differences this evaluation has been
performed by ``stressing'' the two LC to GM assignment algorithms by
simultaneously starting all LCs (128/256) and all GMs (12/25) and then
simulating the resulting configuration over a one hour period.  These
experiments have yielded the following results:
\begin{itemize}
  \item BF yields significantly more homogenous assignments of LCs to
    GMs. For 128 nodes, the no.\ of LCs per GMs varies from 0--30
    (stdev: 10.53) BF and 0--49 (stdev: 15.79) for RR (the means being
    equal since the total no.\ of LCs and GMs are equal). For 256
    nodes, the corresponding values are a range of 0--18 (stdev: 6.62)
    for BF and 0--35 (stdev: 12.22) for RR.\footnote{Here, GMs have
      been assigned 0 LCs if they join the system later than all LCs.}
  \item The cumulated time spent resolving violations is, for both
    configurations, significantly smaller for BF than RR, see
    Fig.~\ref{fig:snoozeBFRRViolation}.
\end{itemize}
From these results, we can clearly infer that BF is significantly
better than RR for these kinds of configuration. We conjecture that BF
should perform at least as good as RR for all configurations (the
proof is left to future work).

\subsubsection{DVMS Analysis}
\AL{if space and time add PajeGN view for DVMS}

\section{Related Work}
\label{sec:related}
\AL[AL]{.25 page}

Several simulator toolkits have been proposed since the last years in
order to adress CC concerns~\cite{CC13, DGSIM, cloudsim,
  icancloud, greencloud}.
They can be classified into two categories: The first one corresponds to ad-hoc simulators
that have been developped to address a particular concern. For
instance, CReST~\cite{CC13} is a discrete event simulation toolkit
built to evaluate Cloud provisioning algorithms. If ad-hoc simulators
enable to provide some trends regarding the bevahiours of the system,
they do consider the implication of the different layers, which can
lead to non representative results at the end. Moreover, such ad-hoc
solutions are developped for one shot and thus, they are not available
for the scientific community. The second category
\cite{icancloud, greencloud, cloudsim} corresponds to more generic cloud simulator
toolkits (\ie they have been designed to adress a majority
of CC challenges). However, they have focused mainly on
the API and not on the model of the different mechanisms of CC
systems.
For instance, CloudSim~\cite{cloudsim}, which has been widely used to validate
algorithms and applications in different scientific publications,
is based on a relatively top-down viewpoint of cloud environments.
That is,  there is no papers that properly validate the different models it
relies on: a migration time is calculated by dividing a VM memory size by a
network bandwidth.
%Such a model cannot correctly simulate many real
%environments where workloads perform substantial memory writes.
 In addition to having inaccuracy weaknesses at the low level, available cloud
simulator toolkits over simplified the model for the virtualization
technologies, leading also to non representation results at the
end. As highlighted several times throughout this document, we chose to
build \vmps on top of \sg in order to benefit fromt its accuracy of
its models related to virtualization abstractions~\cite{Hirofuchi:2013:ALM:2568486.2568524}.

\section{Conclusion}
\label{sec:conclusion}
\AL[AL]{.25 page}
Future work : (i) network changes and other dimensions (I/O), (ii) VM provisioning (i.e.
Adding/removing VMs in the system,


% conference papers do not normally have an appendix


% use section* for acknowledgement
\section{Acknowledgment}
This work is supported by the French ANR project SONGS (11-INFRA-13).
Experiments have been performed using the Grid'5000
experimental testbed, being developed under the INRIA ALADDIN development
 action with support from CNRS, RENATER and several Universities as well as
 other funding bodies (see https://www.grid5000.fr).


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% End:
