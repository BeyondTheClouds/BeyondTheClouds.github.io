\section{Introduction}
\label{sec:intro}
%\AL[AL]{1 page (including the abstract)}

% Although a lot of progress has been made on Cloud Computing (CC)
% system management, \aka Infrastructure-as-a-Service
% toolkits~\cite{moreno:2012}, most of the popular
% solutions~\cite{cloudstack, opennebula, openstack} continue to rely on
%  elementary Virtual Machine (VM) placement
% policies that prevent them from maximizing the usage of CC resources
% while guaranteeing VM resource requirements as defined by Service
% Level Agreements (SLAs).
% %% THE TEXT BELOW COMES FROM ISPA'13
% %%%%%%%
% Typically, a batch scheduling approach is used: VMs (i)~are allocated
% according to user requests for resource reservations, and (ii)~are
% tied to the nodes where they were deployed until their
% destruction. Besides the fact that users often overestimate their
% resource requirements, such static policies are definitely not optimal
% for CC providers, since the effective resource requirements of each
% operated VM may significantly vary during its lifetime.
% %%%%%%%
%  Dynamic strategies such as consolidation, load balancing
% and other SLA-ensuring algorithms have been deeply investigated \cite
% {feller:ccgrid12}, \cite{Hermenier:2009:ECM:1508293.1508300},
% \cite{5715067}, \cite{quesnel:cpe2012}, \cite{5328077},
% \cite{5935254}. \MS{Be more assertive on the motivation: Adrien, which
%   was the article for justification?}

Even if more flexible and often more efficient approaches to the
Virtual Machine Placement Problem (VMPP) have been developed,
%by the research community
most of the popular Cloud Computing (CC) system management
\cite{cloudstack, opennebula, openstack}, \aka IaaS
toolkits~\cite{moreno:2012}, continue to rely on elementary Virtual
Machine (VM) placement policies that prevent them from maximizing the
usage of CC resources while guaranteeing VM resource requirements as
defined by Service Level Agreements (SLAs).
% %% THE TEXT BELOW COMES FROM ISPA'13
% %%%%%%%
Typically, a batch scheduling approach is used: VMs are allocated
according to user requests for resource reservations and tied to
the nodes where they were deployed until their destruction. Besides
the fact that users often overestimate their resource requirements, in
particular for web-services and entreprise information systems
\cite{birke:nom2014, shen:ccgrid2015},
such static policies are definitely not optimal for CC providers,
since the effective resource requirements of each operated VM may
significantly vary during its lifetime.

An important impediment to the adoption of more advanced strategies
such as consolidation, load balancing and other SLA-ensuring
algorithms that have been deeply investigated by the research community
\cite{feller:ccgrid12, Hermenier:2009:ECM:1508293.1508300, 5715067,
  quesnel:cpe2012, 5328077, 5935254} is related to the experimental
processes that have been used to validate them: most VMPP proposals have
been evaluated either by leveraging ad-hoc simulators or small
testbeds. These evaluation environments are not accurate and not
representative enough to (i) ensure their correctness on real
platforms and (ii) perform fair comparisons between them.
%
% %
% \begin{figure}[ht]
% \vspace*{-.2cm}
% \begin{center}
%         \subcapcentertrue
%         \subfigure[Scheduling steps]{
%         \includegraphics[width=.45\linewidth]{figures/scheduling_steps.pdf}
%         \label{fig:scheduling_steps}}
%         \subfigure[Workload fluctuations during scheduling]{
%         \includegraphics[width=.45\linewidth]{figures/workload_fluctuations2.pdf}
%         \label{fig:workload_fluctuations}}
% \vspace*{-.2cm}
% \caption{VM scheduling in a master/worker architecture}
% \end{center}
% \label{fig:scheduling}
% \vspace*{-.2cm}
% \end{figure}
% %
% \MS[AL]{Must we keep the fig.: the arguments of complexity and time
%   requirements are already made in the remainder of the intro. If we
%   keep it, the discussion should be simplified.}
% Each VMPP mechanism is a complex system that can face
% important side-effects during each of its stages: Monitoring the
% resources usages, computing the schedule and applying the
% reconfiguration (see Figure \ref{fig:scheduling_steps}).
% %
% %
% As an example, a single master architecture can lead, \textit{a priori} to
% important drawbacks. First, during the computation and the application
% of a schedule, a single master cannot take into account new VM
% requirement violations. Second, the time needed to apply a new
% schedule can be particularly important: The longer the reconfiguration
% process, the higher the risk that the schedule may be outdated, due to
% the workload fluctuations, when it is eventually applied (see Figure
% \ref{fig:workload_fluctuations}). Finally, a single master node can
% lead to well-known fault-tolerance issues: A group of VMs may be
% temporarily isolated from the master node in case of a network
% disconnection or if the master node crashes.
% %
Implementing each proposal and evaluating it on representative
testbeds in terms of scalability, reliability and varying workload
changes would definitely be the most rigorous way to observe and
propose appropriate solutions for CC production infrastructures.
% and compare  with existing proposals.
However, \textit{in-vivo} (\ie real-world) experiments, if they can be
executed at all, are always expensive and tedious to perform (for
recent reference see~\cite{barker:pitfalls}). They may
even be counterproductive if the observed behaviors are clearly
different from the expected ones.
Consequently, new placement algorithms are continuously proposed
without really identifying the significant benefits of each of them.

In this article, we illustrate in details how \vmps, a dedicated simulation
framework that enables in-depth investigations and fair comparisons of VM placement
algorithms~\cite{vmplaces:europar15}, can help
researchers to mesure and validate the advantages of new proposals.

% To cope with real conditions such as the
% increasing scale of modern data centers and the dynamicity of the
% workloads that are specific to the CC paradigm
% \cite{6838300,shen:ccgrid2015},
%
Built on top the \sg toolkit~\cite{casanova:hal-01017319},
\vmps allows users to study large-scale scenarios that include server crashes and that involve tens of thousands of VMs,
each executing a specific workload that evolves during the
simulation lifetime. Such conditions are mandatory to perform
simulations that are representative enough of CC production platforms
\cite{datacenterAsComputer,birke:nom2014,shen:ccgrid2015}.
%
We chose to base \vmps on \sg since (i) the latter's relevance in
terms of performance and validity has already been
demonstrated~\cite{simgridpub} and (ii) because it has been recently
extended to integrate virtual machine abstractions and an accurate live
migration model~\cite{Hirofuchi:2013:ALM:2568486.2568524}.

% \vmps provides three additional simulation facilities: a more abstract
% representation of hosts and virtual machines, and two frameworks, one
% for the management of load injection  and another one for the
% extraction and analysis of execution traces.

In addition to validating the accuracy of \vmps, we discussed in our
previous work \cite{vmplaces:europar15} a preliminary analysis of
three well-known VMPP approaches:
Entropy~\cite{Hermenier:2009:ECM:1508293.1508300},
Snooze~\cite{feller:ccgrid12}, and DVMS~\cite{quesnel:cpe2012}.
Besides being well-known from the literature, we chose these three
systems as they are built on three different software architecture
approaches: Entropy relies on a centralized model, Snooze on a
hierarchical one and DVMS on a fully distributed one. In this article,
we complete this preliminary analysis by diving into details, showing
the relevance of VMPlaces to compare VM placement strategies, to
identify major limitations and investigate variants that can have a
significant impact on the overall behavior.  in particular, we show in
this article:
\begin{itemize}
  \item The importance of the duration of the reconfiguration phase
    (\ie the step where VMs are relocated throughout the
    infrastructure) in comparison to the computation one (\ie the step
    where the scheduler try to solve the VMPP, see Section
    \ref{sec:vmpp} for a complete definition).
\item The high number of migrations overall.
    % \item The quasi-inexistant impact of failures on the reactivity
    %   for all systems when considering a 6 months failure rate;
  \item The pros and cons of partitioning an infrastructure into small
    or large groups to handle the scheduling process;
  \item The relevance of a reactive strategy in comparison to a
    periodic one.
  \item The moderate impact of node crashes for the hierarchical
%and  distributed
approach even in the presence of high failure rates.
%    \AL[MS]{To be confirmed ?}
  \item The relevance of \vmps to identify the best VM placement
    approaches that have the potential to handle CC production
    infrastructures.
\end{itemize}

The rest of the article is structured as
follow. Section~\ref{sec:vmpp} highlights the importance of the
scalability, reliability and reactivity properties for the VM
Placement problem.  Section~\ref{sec:sg} gives an overview of the \sg
framework on which our proposal is built. Section~\ref{sec:injector}
introduces \vmps and discusses its general functioning. The three
algorithms implemented as use-cases are presented in
Section~\ref{sec:vm-schedulers} and evaluated in
Section~\ref{sec:experiments}. Section~\ref{sec:related} and
Section~\ref{sec:conclusion} present, respectively, related work as
well as a conclusion and future work.

\section{The VM Placement Problem}
\label{sec:vmpp}

A VMPP can be summarized in three-steps (see Figure
\ref{fig:scheduling_steps}): monitoring the resources usages,
computing a new schedule each time is it needed and applying the
resulting reconfiguration plan (\ie performing VM migration and
suspend/resume operations to switch to the new placement solution).
% that are mandatory to solve resource violations while optimizing resource usages).

\begin{figure}[ht]
\vspace*{-.2cm}
\begin{center}
        \subcapcentertrue
        \subfigure[Scheduling steps]{
        \includegraphics[width=.45\linewidth]{figures/scheduling_steps.pdf}
        \label{fig:scheduling_steps}}
        \subfigure[Workload fluctuations during scheduling]{
        \includegraphics[width=.45\linewidth]{figures/workload_fluctuations2.pdf}
        \label{fig:workload_fluctuations}}
\vspace*{-.2cm}
\caption{VM scheduling Phases}
\end{center}
\label{fig:scheduling}
\vspace*{-.2cm}
\end{figure}
%

VMPP solutions stand and fall with their scalability, reliability and
reactivity of properties (\ie the time to solve a SLA violations), because they have to maintain a placement
that satisfies the requirements of all VMs while optimizing the usage
of CC resources. For instance, a naive implementation of a
master/worker approach as described in
Figure~\ref{fig:scheduling_steps} would prevent workload fluctuations
to be taken into account during the computation and the application of
a schedule, potentially leading to artificial violations (\ie resource
violations that are caused by the VMPP mechanism). In other words, the
longer each phase,
% the longer the reconfiguration process,
the higher the risk that the schedule may be outdated when it is
computed or eventually applied (see the different loads during the
three phases in Figure \ref{fig:workload_fluctuations}). Similarly,
servers and network crashes can impede the detection and resolution of
resource violations if the master node crashes or if a group of VMs is
temporarily isolated from the master node.

VMPP solutions can only be reasonably evaluated if their behavior in
the presence of such adverse events can be analyzed. Providing a
framework that facilitates such studies and increases their
reproducibility is the main objective of \vmps.

\section{Simgrid, a generic toolkit}
\label{sec:sg}
%\AL[AL]{0.5page}

We now briefly introduce the toolkit on which \vmps is based.  \sg is
a toolkit for the simulation of potentially complex algorithms
executed on large-scale distributed systems.  Developed for more than
a decade, it has been used in a large number of studies described in
more than 100~publications.  Its main characteristics are the
following:
\begin{itemize}
  \item Extensibility: after Grids, HPC and P2P systems, \sg has been
    recently extended with abstractions for virtualization
    technologies (\ie Virtual Machines including a live migration
    model \cite{Hirofuchi:2013:ALM:2568486.2568524}) to allow users to
    investigate Cloud Computing challenges \cite{lucas:cloud2014}.
  \item Scalability: it is possible to simulate large-scale scenarios;
    as an example, users can simulate applications composed of
    2~million processors and an infrastructure composed of
    10,000~servers~\cite{casanova:hal-01017319}.
%hosting more than 100,000~VMs on a computer with 16~GB of memory. \MS[AL]{Add a ref.}
  \item Flexibility: it enables simulations to be run on arbitrary
    network topologies under dynamically changing computations and
    available network resources.
  \item Versatile APIs: users can leverage \sg through easy-to-use
    APIs for~C and~Java.
\end{itemize}

To perform simulations, users should develop a \emph{program} and
define a \emph{platform} file and a \emph{deployment} file. The
\emph{program} leverages, in most cases, the \sg MSG API that allows
end-users to create and execute \sg abstractions such as processes,
tasks, VMs and network communications. The \emph{platform} file
provides the physical description of each resource composing the
environment and on which aforementioned computations and network
interactions will be performed in the \sg world.
% (host, CPU capacity, network topology and link capacities, etc.)
The \emph{deployment} file is used to launch the different \sg
processes defined in the \emph{program} on the different nodes.
% (at least the mapping between one process and one host is mandatory
% to start the simulation)
Finally, the execution of the program is orchestrated by the \sg
engine that internally relies on an constraint solver to correctly
assign the amount of CPU/network resources to each \sg abstraction
during the entire simulation.

\sg provides many other features such as model checking, the
simulation of DAGs (Direct Acyclic Graphs) or MPI-based
applications. In the following, we only give a brief description of
the virtualization abstractions that have been recently implemented
and on which \vmps relies on (for further information regarding
\sg see~\cite{casanova:hal-01017319}).

The VM support has been designed so that all operations that can be
performed on a host can also be performed inside a VM. From the point
of view of a \sg Host, a \sg VM is an ordinary task while from the
point of view of a task running inside a \sg VM, a VM is considered as
an ordinary host.
% below the task.
\sg users can thus easily switch between a virtualized and
non-virtualized infrastructure.  Moreover, thanks to MSG API
extensions, users can control VMs in the same manner as in the real
world (\eg create/destroy VMs; start/shutdown, suspend/resume and
migrate them).
% TODO: Not addressed
%\AL{Shoudl we talk about over-provisionning limitations}
For migration operations, a VM live migration model implementing the
precopy migration algorithm of Qemu/KVM has been integrated into \sg.
This model is the only one that successfully simulates the live
migration behavior by taking into account the competition arising in
the presence of resource sharing as well as the memory refreshing rate
of the VM, thus determining correctly the live migration time as well
as the resulting network
traffic~\cite{Hirofuchi:2013:ALM:2568486.2568524}.
%
%\AL[AL]{Add more details regarding live migration in order to reply to
%the Mario's remark (not clear enoug)}
%
These two capabilities were mandatory to build the VM placement
simulator toolkit.

\section{VMPlaceS}
\label{sec:injector}
%\AL[AL]{1.5 page}

The purpose of \vmps is to deliver a generic tool to evaluate new VM
placement algorithms and offer the possibility to compare
them. Concretely, it supports the management of VM creations, workload
fluctuations as well as node apparitions/removals.  Researchers can
thus focus on the implementation of new placement algorithms and
evaluate how they behave in the presence of changes that occur during
the simulation.
%
\vmps has been implemented in Java by leveraging the messaging API
(MSG) of \sg. Although the Java layer has an impact of the efficiency
of \sg, we believe its use is acceptable because Java offers important
benefits to researchers for the implementation of advanced scheduling
strategies, notably concerning the ease of implementation of new
strategies. As examples, we reimplemented the Snooze proposal in Java
and the DVMS proposal using Scala/Java.

In the following we give an overview of the framework and describe its
general functioning.% and how researchers can develop new algorithms

\subsection{Overview}
\label{sec:overview}

From a high-level view, \vmps performs a simulation in three phases:
(i) initialization (ii) injection and (iii) trace analysis (see Figure
\ref{fig:workflow}).  The initialization phase corresponds to the
creation of the environment, the VMs and the generation of the queue
of events that may represent, \eg load changes.  The
simulation is performed by at least two \sg processes, one executing
the \emph{injector}
% which constitutes the generic part of the framework,
and a second one executing the to-be-simulated
\emph{scheduling algorithm}. During the simulation the scheduling
strategy is evaluated by taking into account the different events
played by the injector. Currently, the supported events are VM CPU
load changes and node apparitions/removals that we use to simulate node crashes.
%
The last phase consists in the analysis of the collected traces in
order to gather the results of the simulation, notably by means of the
generation of figures representing, \eg resource usage statistics.

\begin{figure}
  {\centering ~\includegraphics[width=.95\linewidth]{figures/VMPlaceS-workflow.png}}
  \caption{\vmps's Workflow}
  \label{fig:workflow}
{\small Gray parts correspond to the generic code while the white one
  must be provided by end-users. The current version is released with
  three different schedulers (centralized/hierarchical and distributed).}
\end{figure}

% As we describe in the next section, additional events can be easily added.
%\AL[AL]{Make two figures: a architectural one (i.e. Injector vs
 % schedulers pool and one chronological.}
%\MS{Yes, the figures are important. They could also be useful to
 % partially provide a more abstract explanation.}

Regarding the non-generic part, users must develop their scheduling
algorithm by leveraging the \sg messaging API and a more abstract
interface that is provided by \vmps and consists of the classes
\texttt{XHost}, \texttt{XVM} and \texttt{SimulatorManager} classes.
The two former classes respectively extend \sg's \texttt{Host} and
\texttt{VM} abstractions while the latter controls the interactions
between the different components of the VM placement simulator.
Throughout these three classes, users can inspect, at any time, the
current state of the infrastructure (\ie the load of a host/VM, the
number of VMs hosted on the whole infrastructure or on a particular
host, check whether a host is overloaded,
etc.)

%We have used \vmps in order to analyze three
%scheduling mechanisms, cf.\ Sec.~\ref{sec:vm-schedulers}, that
%represent three different software architecture models: centralized,
%hierarchical and fully-distributed models for VM placement.
%% TODO
%\MS{The
%  following point is too low-level and should not come here} Although
%we do not discuss that point due to space constraints, we emphasize
%that these three mechanisms enable us to deliver concrete examples of
%how the deployment file of \sg is automatically generated by leveraging
%a generic python script.  \AL{We should highlight that point in the
%  README.org}

%\begin{itemize}
%\item Entropy \cite{Hermenier:2009:ECM:1508293.1508300}, a centralized approach using a constraint programming approach to solve the placement/reconfiguration VM problem;
% \item Snooze \cite{feller:ccgrid12}, a hierarchical approach where
%   each manager of a group invokes Entropy to solve the
%  placement/reconfiguration VM problem. It is noteworthy that in
%   \cite{feller:ccgrid12}, Snooze is using a specific heuristic to solve the placement/reconfiguration VM problem. As the sake of simplicity, we have simply reused the entropy scheduling code.
%\item  DVMS \cite{quesnel:cpe2012}, a distributed approach that dynamically partitions the system and invokes Entropy on each partition.
% \end{itemize}

\subsection{Initialization Phase}

In the beginning, \vmps creates $n$ VMs and assigns them in a
round-robin manner to the first $p$ hosts defined in the platform
file.  The default platform file corresponds to a cluster of $h+s$
hosts, where $h$ corresponds to the number of hosting nodes and $s$ to
the number of services nodes. The values $n$, $h$ and $s$ constitute
input parameters of the simulations (specified in a Java property
file).
%% TODO
% \AL[AL]{Update the size of the cluster autonomically by
%  leveraging p + s}
These hosts are organized in form of topologies, a cluster topology
being the most common ones. It is possible, however, to define more
complex platforms to simulate, for instance, scenarios involving
federated data centers.
%Note that $s$ can be equals to 0 if the
%scheduling strategy is directly executed on the hosting nodes.

Each VM is created based on one of the predefined VM classes. A VM
class corresponds to a template specifying the VM attributes and its
memory footprint. Concretely, it is
% described as
% \texttt{nb\_cpu:ramsize:net\_bw:mig\_speed:mem\_speed}
defined in terms of five parameters: the number of cores
\texttt{nb\_cpus}, the size of the memory \texttt{ramsize}, the
network bandwidth \texttt{net\_bw}, the maximum bandwidth available
to migrate it \texttt{mig\_speed} (generally equal to
\texttt{net\_bw}) and the maximum memory update speed
\texttt{mem\_speed} when the VM is consuming 100\% of its CPU
resources. As pointed out in Section \ref{sec:sg}, the memory update
speed is a critical parameter that governs the migration time as well
as the amount of transferred data. By giving the possibility to define
VM classes, \vmps allows researchers to simulate different kinds of
workload (\ie memory-intensive vs non-intensive workloads), and thus
analyze more realistic Cloud Computing problems.  Available classes
are defined in a specific text file that can be modified according to
the user's needs.
%\MS{Follows a low-level mechanism!}
% TODO not addressed yet. This text should appear in the README
%At creation time
%of a VM, a process selects one class (\ie one line) in the file
%randomly. Hence, if a user wants to favor a specific class, he can
%simply repeat the line of the class several times.
%
Finally, all VMs start with a CPU consumption of 0 that will evolve
during the simulation in terms of the injected load as explained
below.

Once the creation and the assignment of VMs completed, \vmps spawns at
least two SG processes, the \emph{injector} and the launcher of the
selected scheduler.  The first action of the \emph{injector} consists
in creating the different event queues and merge them into a global
one that will be consumed during the second phase of the simulation.
For now, we generate two kinds of event: \emph{CPU load} and
\emph{node crash} events.
%\todo{Before apparitions/ removals}
% The former consists in changing the load of a VM by creating and
% assigning a new \sg task in the VM while the second aims at
% simulating crashes.
%
% Changing the load of a VM has a direct impact of its memory update
% speed and thus on the time to migrate it between two hosts.
Regarding \emph{CPU loads}, the event queue is generated in order to change the
load of each VM every $t$ seconds on average. $t$ is a random variable
that follows an exponential distribution with rate parameter
$\lambda_t$ while the CPU load of a VM evolves according to a Gaussian
distribution defined by a given mean ($\mu$) as well as a given
standard deviation ($\sigma$). $t$, $\mu$ and $\sigma$ are provided as
input parameters of a simulation. As the CPU load can fluctuate
between 0 and 100\%, \vmps prevents the assignment of nonsensical
values when the Gaussian distribution returns a number smaller than 0
or greater than 100. Although this has no impact on the execution of
the simulation, we emphasize that this can reduce/increase the
effective mean of the VM load, especially when $\sigma$ is high.
Hence, it is important for users to specify appropriate values.
%% TODO
%\AL[AL]{A binomial law would have solved this issue: too late too bad :(}
%Although this can have an impact on the
%effective mean, especially when $\sigma$ is high, we believe it was
%non appropriated to request it is easier for end-users to specify $\mu$ and
%$\sigma$ parameters than
Furthermore, each random process used in \vmps is initialized with a
seed that is defined in a configuration file. This way, we can ensure
that different simulations are reproducible and may be used to
establish fair comparisons.

While a recent study on the characterization of production CC
workloads~\cite{birke:nom2014} advocate the use of exponential
distributions to capture both CPU usages and the temporal variability,
we believe that a Gaussian distribution is more appropriate to simulate small periods of CC
platforms (\ie less than 2 hours). In addition to being more expressive for
researchers (\ie it is simpler to define $\mu$ and $\sigma$ than
finding the right $\lambda$), the exponential law does not allow \vmps
to concentrate the possible changes around a common value for a
particular period, which corresponds to the normal behavior of CC
workloads at a certain moment of the day~\cite{shen:ccgrid2015}. In
other words, the load of a CC workload significantly differs at the
scale of the day but not at the scale of the hour. Finding the right
distribution that will enable \vmps to simulate CC production
platforms over longer periods (\ie days) is part of our planned future
work.

Regarding \emph{node crashes}, the event queue is generated in order to turn off a
node every $f$ seconds on average for a duration of $d$ seconds~\cite{datacenterAsComputer}.
Similarly to the $t$ value above, $f$ follows an exponential
distribution with rate $\lambda_f$.
$f$ and $d$ are also provided as
input parameters of a simulation.

Finally, we highlight that adding new events can easily be done by simply defining new event Java
classes implementing the \texttt{InjectorEvent} interface and by
adding the code in charge of generating the associated queue. Such a
new queue will be merged into the global one and its events will then be
consumed similarly to other ones during the \emph{injector phase}.
We are currently integrating new events in order to simulate network
and disk usage events.

\subsection{Injector Phase}

Once the VMs and the global event queue are ready, the evaluation of
the scheduling mechanism can start. First, the injector process
iteratively consumes the different events that represent, for now,
load changes of a VM or turning a node off or on. Changing the load of
a VM corresponds to the creation and the assignment of a new \sg task
in the VM. This new task has a direct impact on the time that will be
needed to migrate the VM as it increases or decreases the current CPU
load and thus % the percentage of
its memory update speed.
% \MS[AL]{Is the above paragraph clear enough?}
% that is indicated by the \texttt{mem\_speed}
%% parameter given in the class description.
When a node is turning off, the VMs that were running on that node are
temporarily discarded, \ie they are hidden and cannot be accessed
until the node comes back to life. This way, the scheduler cannot
handle them.
 %\AL[AL, MS, JP]{This is ugly but unfortunately the true,
 % it will be better to reassign those VMs on other nodes, but which
 % one?  }
We leave for future work other approaches that can better
match realistic scenarios such as turning off the VMs and
reprovisioning them on other nodes.
%

As defined by the scheduling algorithm, VMs will be suspended/resumed
or relocated on the available hosts to meet scheduling objectives and
SLA guarantees.  Note that users must implement the algorithm in
charge of solving the VMPP but also the code in charge of applying
reconfiguration plans by invoking the appropriate methods available
from the \texttt{SimulatorManager} class. This step is essential as
the reconfiguration cost is a key element of dynamic placement
systems.

% \MS[AL]{maybe it is better to prevent the access to Xhost
%   and XVM methods that can change the Simulator States. Hence, we
%   should enforce the access only through the SimulatorManager class?
%   What do you think? Yes, would be cleaner. Can we just present the
%   interface as such? Or not talk about the direct possibility?}
Last but not least, it is noteworthy that \vmps really invokes the
execution of each scheduling strategy in order to get the effective
reconfiguration plan.  That is, the computation time that is observed
is not simulated but corresponds to the effective one, only the
workload inside the VMs and the migration operations are simulated in
\sg. It is hence mandatory to propagate the reconfiguration time into
the \sg engine.%
% The following is IMO a technical detail
% by invoking a \texttt{wait} call of the MSG interface.

\subsection{Trace Analysis}
\label{subsec:traces-analysis}

The last step of \vmps consists in analyzing the information that has
been collected during the simulation.
% in order to understand and compare the behavior of the different
% algorithms.
This analysis is done in two steps. First, \vmps records several
metrics related to the platform utilization throughout the simulation
by leveraging an extended version of \sg's TRACE
module\footnote{\url{http://simgrid.gforge.inria.fr/simgrid/3.12/doc/tracing.html}}.
This way, visualization tools that have been developed by the \sg
community, such as PajeNG~\cite{pageng:www}, may be used. Furthermore,
our extension enables the creation of a trace file in the JSON file
format, which is used to generate several figures using the R
statistical environment~\cite{R:Bloomfield:2014} about the resource
usage during the simulation.

By default, \vmps records the load of the different VMs and hosts, the
appearance and the duration of each violation of VM requirements in
the system, the number of migrations, the number of times the
scheduler mechanism has been invoked and the number of times it
succeeds or fails to resolve non-viable configurations.
%
Although these pieces of information are key elements to understand
and compare the behavior of the different algorithms, we emphasize
that the TRACE API enables the creation of as many variables as
necessary, thus allowing researchers to instrument their own algorithm
with specific variables that record other pieces of information.

\section{Dynamic VMPP Algorithms}
\label{sec:vm-schedulers}
To illustrate the interest of \vmps, we implemented three dynamic VM
placement mechanisms respectively based on the Entropy
\cite{Hermenier:2009:ECM:1508293.1508300}, Snooze
\cite{feller:ccgrid12}, and DVMS \cite{quesnel:cpe2012} proposals. For the three
implementations, we chose to use the latest VMPP solver that has been
developed as part of the Entropy
framework~\cite{hermenier:cp11}.

%
Giving up consolidation
optimality in favor of scalability, this algorithm provides a ``repair
mode'' that enables the correction of VM requirement violations. The algorithm considers that a host is
overloaded when the VMs try to consume more than 100\% of the CPU
capacity of the host. In such a case, the algorithm looks for
an optimal viable configuration until it reaches a predefined timeout.
The optimal solution is a new placement that satisfies
the requirements of all VMs while minimizing the cost of the
reconfiguration.
Once the timeout has been triggered, the algorithm returns
the best solution among the ones it finds and applies the associated
reconfiguration plan by invoking live migrations in the simulation
world.

%
Although using the Entropy VMPP solver
implies a modification from the original Snooze proposal,  we
highlight that our goal is to illustrate the capabilities of \vmps and
thus we believe that such a modification is acceptable as it does not
change the global behavior of Snooze. Moreover by
conducting such a comparison, we also investigate the pros and cons of
the three  architecture models on which these proposals rely on (\ie centralized, hierarchical and
distributed).

%
Before discussing the simulation results, we
describe in this section an overview of the three implemented systems.
We highlight that the extended abstractions for hosts (\texttt{XHost})
and VMs (\texttt{XVM}) as well as the available functions of the \sg
MSG API enabled us to develop them in a direct and natural manner.


\subsection{Entropy-based Centralized Approach}
\label{subsec:entropy}
The centralized VM placement mechanism consists in one single \sg
process deployed on a service node. This process implements a simple loop that
iteratively checks the viability of the current configuration by
invoking every $p$ seconds the aforementioned VMPP solver. $p$ is
defined as an input parameter of the simulation.

% \AL{Should we explain the issue right now or not if we add VMPP section}
% Indeed, during
% the computation and the application of a schedule, the algorithm does
% not enforce QoS properties anymore, and thus cannot react quickly to
% violations. Second, since the manipulation of VMs is costly, the time
% needed to apply a new schedule is particularly important: The longer
% the reconfiguration process is, the higher is the risk that the schedule may
% be outdated, due to the workload fluctuations, when it is eventually
% applied.
% \vmps enables researchers to investigate such concerns in-depth.

As the Entropy proposal does not provide a specific mechanism for the
collect of resource usage information but simply uses an external tool
(namely ganglia), we had two different ways to implement the monitoring to
process:  either by implementing additional asynchronous transmissions
as a real implementation of the necessary state updates would proceed
or, in a much more lightweight manner, through direct accesses by the
aforementioned process to the states of the hosts and their respective
VMs. While the latter does not mimic a real implementation closely, it
can be harnessed to yield a valid simulation: overheads induced by
communication in the ``real'' implementation, for instance, can be
easily added as part of the lightweight simulation. We have
implemented this lightweight variant for the monitoring

Regarding fault tolerance, similarly to the Entropy proposal, our
implementation does not provide any failover mechanism.

Finally, as mentioned in Section \ref{subsec:traces-analysis}, we monitor, for each iteration,
whether the VMPP solver succeeds or fails. In case of success, \vmps
records the number of migration that has been performed, the time it
took to apply the reconfiguration and whether
the application of the reconfiguration plan led to new violations.

\subsection{Snooze-based Hierarchical Approach}
\label{subsec:snooze}
\input{snooze.tex}

\subsection{DVMS-based Distributed Approach}
\label{subsec:dvms}
% TODO Not adressed
%\AL[AL]{Check who write that part, If Flavien did it, then add him as
%  an author}
\input{dvms}

\section{Experiments}
\label{sec:experiments}
% %\AL[JP,AL,MS]{2 pages}
% In this section, we, first, analyze the accuracy of \vmps by comparing
% the  results of an Entropy execution through simulations and
% \textit{in-vivo} experiments. This first experiment enables us to
% confirm the expected behavior of \vmps Second, we present and discuss
% our analysis of the three algorithms previously described.
% We show that the performances of the hierarchical approach could
% reached the distributed ones through minor changes.

% \subsection{Accuracy Evaluation of \vmps}
% \label{subsec:accuracy}
% In order to validate the accuracy of \vmps, we implemented a dedicated
% version of our
% framework\footnote{\url{https://github.com/BeyondTheClouds/G5K-VMPlaceS}}
% on top of the Grid'5000 testbed and compared the execution of the
% Entropy strategy invoked every 60 seconds over a 3600 seconds period
% in both the simulated and real world. Regarding the \textit{in-vivo}
% conditions, experiments have been performed on top of the Graphene
% cluster (Intel Xeon X3440-4 CPU cores, 16 GB memory, a GbE NIC, Linux
% 3.2, Qemu 1.5 and SFQ network policy enabled) with 6 VMs per node.
% Each VM has been created as one of the 8 VM predefined classes. The
% template was 1:1GB:1Gbps:1Gbps:X, where the memory update speed X was
% a value between 0 and 80\% of the migration bandwidth (1Gbps) in steps
% of 10. Starting from 0\%, the load of each VM varied according to the
% exponential and the Gaussian distributions previously described. The
% parameters were $\lambda$ = No VMs/300 and $\mu$= 60, $\sigma$ = 20.
% Concretely, the load of each VM varied on average every 5 min in steps
% of 10 (with a significant part between 40\% and 80\%). A dedicated
% \texttt{memtouch} program\cite{Hirofuchi:2013:ALM:2568486.2568524} has
% been used to stress both the CPU and the memory accordingly. Regarding
% the simulated executions, \vmps has been configured to reflect the
% \textit{in-vivo} conditions. In particular, we configured the network model of
% SimGrid in order to cope with the network performance of the Graphene
% servers that were allocated to our experiment (6 MBytes for the TCP
% gamma parameter and 0.88 for the bandwidth corrective simulation
% factor).

% Fig.~\ref{fig:usecase-vivosimu} shows the cost of the two phases of
% the Entropy algorithm for each invocation when considering 32~PMs and
% 192~VMs through simulations (top) and in reality (bottom). At
% coarse-grained, we can see that simulation results successfully
% followed the in-vivo ones. During the first hundreds seconds, the cluster did not
% experience VM requirement violations because the loads of VM were
% still small (\ie Entropy simply validated that the curent placement
% statisfied all VM requirements). At 540 seconds, Entropy started to
% detect non viable configurations and performed reconfigurations.
% Diving into details, the difference between the \textit{simulated} and
% \textit{in-vivo} reconfiguration time fluctuated between 6\% and 18\%
% (median was around 12\%) during the experiment. The worst case, \ie 18\%, was reached when
% multiple migrations were performed simultaneously on the same
% destination node. In this case and even if the SFQ network policy was
% enabled, we discovered that in the reality the throughput of migration
% traffic fluctuated when multiple migration sessions simultaneously
% shared the same destination node. We confirmed this point by analyzing
% TCP bandwidth sharing through \texttt{iperf} executions. We are
% currently investigating with the \sg core-developpers how we can
% integrate this phenomenon into the live-migration model. Howver, as a
% migration lasts less than 15 seconds in average, we believe that that
% the current simulation results are sufficiently accurate to capture
% performance trends of placement strategies.

% \begin{figure}[hbt]
% \centering
% \includegraphics[width=0.39\textwidth]{./figures/simu-vivo-32PM-192VM-6020.png}
% \caption{Comparison between simulated and \textit{in-vivo} Executions}
% \flushleft\scriptsize{The red parts correspond to the time periods where Entropy checks the viability
% of the current configuration and compute a new viable configuration if necessary.
% The black parts correspond to the reconfiguration phases (\ie when
% Entropy performs the migrations of the VM to reach the new
% configuration that solves the QoS issues).}
% \label{fig:usecase-vivosimu}
% %% TH \vspace*{-.3cm}
% \end{figure}

% As an example, we noticed that applying the
% reconfiguration plan was much more time-consuming than computing it. This result that has been correctly reported by the simuations means that VMPP also needs
% to address the way of shorten reconfiguration phases, not only that of
% computing ones.
% % This is rather important as most relocation
% % algorithms try to reduce the computation phase instead of focusing on the
% % reconfiguration one.
% Leveraging \vmps will enable researchers to observe such key points
% without facing with the burden of conducting large scale
% \textit{in-vivo} experiments. We illustrate such an advantage in the
% following section.

% \subsection{A First Use-Case:  Comparison of Entropy, Snooze and DVMS}
% \label{subsec:first-usecase}
% %\AL{Il faudra parler du nombre de migrations qui est egalement une
% %  m√©trique pertinente. Plusieurs algorithms tentent de reduire cette
% %  metrique }
% %\AL[AL]{Il faudra mettre des snapshots de PajeNG}


% % Evaluation of VMPlaceS on Grid'5000: simulations were running on one
%server.
In this section, we discuss the results of the simulations we
performed on the Entropy, Snooze and DVMS strategies. First, we
present a general study analyzing the violation times, the duration of
the computation and reconfiguration phases and the number of migration
performed for the standard implementations (\aka vanilla code).
Second, we examine some variants of and improvements to Snooze and
DVMS that have been studied easily thanks to \vmps. We highlight that
the accuracy of the results have been validated in our previous
work~\cite{vmplaces:europar15}.

 \subsection{Comparing Entropy, Snooze and DVMS (Vanilla Impl.)}
% \label{subsec:first-usecase}
%
\subsubsection{Experimental Conditions}

All simulations have been performed on the % Lyon clusters of the
Grid'5000 testbed~\cite{grid5000}.  Each execution was running on a
dedicated server, thus avoiding interferences between simulations and
ensuring reproducibility between the different invocations.

% Scripts: automation of the deployment, running of simulations and the collect
% of results.

% It enables us to run a large number of simulations, with several variants
% of the scheduling algorithm.

\vmps has been configured in order to simulate an homogeneous
infrastructure of PMs composed of 8 cores, 32~GB of RAM and 1~Gpbs
Ethernet NIC. To enable fair comparisons between the three strategies,
the scheduling resolver only considered 7 cores (\ie one was devoted
to run the Snooze GL or the DVMS processes). Dedicating one core for
the host OS and other administrative processes is something which is
quite common and, as we believe, acceptable as part of our
experimental methodology.

We conducted simulations in order to study infrastructures composed of
128, 256, 512, 1024,
% and 2048 PMs, \AL[JP]{Add 2048 if succeeded}
for each PM number hosting 10 times as many VMs. Additional simulated PMs have
been provided to execute the Entropy and Snooze service nodes on
distinct nodes. For Snooze, one GM has been created for every 32 LCs
(\ie PMs). Entropy and Snooze are invoked every 30 seconds. Finally,
it is noteworthy that no service node had to be provisioned for DVMS
as a DVMS process is executed directly on top of the hosting nodes.

In order to cope with real DC conditions, we defined the parameters
for node crashes to simulate a fault on average every 6 months for a
duration of 300 seconds. These values correspond to the Mean Time To
Failure (MTTF) and the Mean Time To Repair (MTTR) of a Google DC
server~\cite[pp. 107-108]{datacenterAsComputer}. We underline that at
the scale we performed our simulations such a crash ratio was not
sufficient to impact the behavior of the scheduling policies.
Dedicated simulations were mandatory to study the influence of node
crashes with an higher failure rate (see Section \ref{subsubsec:node-crashes})

Regarding the virtual machines, ten VMs have
been initially launched on each simulated PM.  Each VM has been
created as one of the 8 VM predefined classes. The template was
1:1GB:1Gbps:1Gbps:X, where the memory update speed X was a value
between 0 and 80\% of the migration bandwidth (1Gbps) in steps of
10. Starting from 0\%, the load of each VM varied according to the
exponential and the Gaussian distributions previously described. The
parameters were $\lambda$~=~No VMs/300 and $\mu$= 60, $\sigma$ = 20.
Concretely, the load of each VM varied on average every 5 min in steps
of 10 (with a significant part between 40\% and 80\%). The stationary
state was reached after 20 min of the simulated time with a global
load of 85\% as depicted in Fig. \ref{fig:load_figure}. To accelerate
the simulations, we have chosen to limit the simulated time to 1800
seconds. It is noteworthy that the consolidation ratio, \ie the number
of VMs per node, has been defined to generate a sufficient number of
violations. We discovered that under a global load of 75\%, the
simulated infrastructure almost did not face VM violations with our selected
Gaussian distribution. Such a result is rather satisfactory as it can
explain why most production DCs target such an overall utilization
rate.\footnote{\url{http://www.cloudscaling.com/blog/cloud-computing/amazons-ec2-generating-220m-annually/}}

All configuration files used to perform the discussed simulations can
be downloaded from the \vmps repository.\footnote{\url{http://beyondtheclouds.github.io/VMPlaceS/}}

\subsubsection{General  Analysis}
\label{subsec:general-comparison}

\begin{figure}[bp]
\subcapcentertrue
\subfigure[Infrastructure load]{\includegraphics[width=4cm]{./figures/experiments/1024-hierarchical.pdf}\label{fig:load_figure}}
\subfigure[Cumulated Violation Time]{\includegraphics[width=4cm]{./figures/experiments/violation_time.pdf}\label{fig:cumulated_violation}}
\caption{Simulation Results - 10 VMs per node (VM load: $\mu=60$ and $\sigma=20$)}
\label{fig:simulation-overview}
\end{figure}

%%%

% \begin{figure}[ht]
% \centering
% \begin{minipage}[c]{.48\textwidth}
%     \includegraphics[width=.48\textwidth]{figures/experiments/1024-hierarchical.pdf}
%     \caption{Evolution of the global CPU load}
% \label{fig:load_figure}
% \end{minipage}
% \begin{minipage}[c]{.48\textwidth}
%      \includegraphics[width=.48\textwidth]{figures/experiments/violation_time.pdf}
%      \caption{Cumulated Violation Duration}
% \label{fig:cumulated_violation}
% \end{minipage}
% \end{figure}

%To enable a fair comparison of several scheduling algorithms, the simulations is
%associated with a workload simulator which simulates a virtual workload in each
%simulated virtual machines, thus enabling the study of the behaviour of the
%simulated algorithms.
%The figure \ref{fig:load_figure} illustrates the global load.
%To accelerate the simulations, we have chosen
%parameters that cause the workload to reach a stationary phase after 1200
%seconds of simulations, thus enabling us to limit the simulation duration to
%1800 seconds.

Fig.~\ref{fig:cumulated_violation} presents the cumulated violation
time for each placement policy while
Tables~\ref{table:detailed_violation_time},
\ref{table:detailed_computation_time} and
\ref{tab:detailed_reconf_time} give more details by presenting the
mean and the standard deviations of the duration of, respectively, the
violations, the computation and reconfiguration phases. As
anticipated, the centralized approach did not scale and became almost
counterproductive for the largest scenario in comparison to a system
that did not use any dynamic scheduling strategy. The more nodes Entropy has to
monitor, the less efficient it is on both the computation and
reconfiguration phases. Regarding the computation, the VMPP is a
NP-Hard problem and thus it is not surprising that it takes more time
to resolve larger problems. Regarding the reconfiguration, as Entropy
has to solve much more violations simultaneously, the reconfiguration plan
is more complex for large scenarios, including several migrations
coming from and going to the same nodes. Such reconfiguration plans
are non optimal as they increase the bottleneck effects at the network
level of each involved PM. Such a simulated result is valuable as it confirms
that reconfiguration plans should avoid as much as possible such
manipulations.

% \begin{table}[ht]
% \centering
%     {\scriptsize \begin{tabular}{|P{10mm}@{\:}||@{\:}c@{\:}|@{\:}c@{\:}|@{\:}c@{\:}||@{\:}c@{\:}|@{\:}c@{\:}|@{\:}c@{\:}|}
%       \thickhline
%       \textbf{Infrastructure Size}
%         & \multicolumn{4}{c@{\:}||@{\:}}{\textbf{Algorithm}}
%           \Tstrut \\
%          \hfill & ~Without~ & ~Centralized~ & ~Hierarchical~ & Distributed \Bstrut \\
%       \thickhline
%         128 nodes  & 79.20 $\pm$  89.94 & 21.26 $\pm$ 13.55 & 21.07 $\pm$ 12.32 &   9.55 $\pm$ 2.57 \\
%         256 nodes  & 70.86 $\pm$  87.56 & 40.09 $\pm$ 24.15 & 21.45 $\pm$ 12.10 &   9.58 $\pm$ 2.51 \\
%         512 nodes  & 65.63 $\pm$  65.56 & 55.63 $\pm$ 42.26 & 24.54 $\pm$ 16.95 &   9.57 $\pm$ 2.67 \\
%         1024 nodes & 85.90 $\pm$ 101.51 & 81.57 $\pm$ 86.59 & 29.01 $\pm$ 38.14 & \:9.61 $\pm$ 2.54
%       \Rstrut  \\ \hline
%       \thickhline
%   \end{tabular} }
% \caption{Means $\pm$ Std deviations of violation durations.}
% \label{table:detailed_violation_time}
% \end{table}

% \begin{table}[ht]
% \centering
%     {\scriptsize \begin{tabular}{|P{27mm}@{\:}||@{\:}c@{\:}|@{\:}c@{\:}|@{\:}c@{\:}|}
%       \thickhline
%       \textbf{Infrastructure Size}
%         & \multicolumn{3}{c@{\:}|}{\textbf{Algorithm}}
%           \Tstrut \\
%          \hfill  & ~Centralized~ & ~Hierarchical~ & Distributed \Bstrut \\
%       \thickhline
%         128 nodes   & 21.26 $\pm$ 13.55 & 21.07 $\pm$ 12.32 &   9.55 $\pm$ 2.57 \\
%         256 nodes   & 40.09 $\pm$ 24.15 & 21.45 $\pm$ 12.10 &   9.58 $\pm$ 2.51 \\
%         512 nodes   & 55.63 $\pm$ 42.26 & 24.54 $\pm$ 16.95 &   9.57 $\pm$ 2.67 \\
%         1024 nodes  & 81.57 $\pm$ 86.59 & 29.01 $\pm$ 38.14 & \:9.61 $\pm$ 2.54
%       \Rstrut  \\ \hline
%       \thickhline
%   \end{tabular} }
% \caption{Duration of violations ($Med \pm \sigma$)}
% \label{table:detailed_violation_time}
% \end{table}

\begin{table}[t]
\centering
    {\scriptsize \begin{tabular}{|P{27mm}@{\:}||@{\:}c@{\:}|@{\:}c@{\:}|@{\:}c@{\:}|}
      \thickhline
      \textbf{Infrastructure Size}
        & \multicolumn{3}{c@{\:}|}{\textbf{Algorithm}}
          \Tstrut \\
         \hfill  & ~Centralized~ & ~Hierarchical~ & Distributed \Bstrut \\
      \thickhline
          128 nodes &  23.30 $\pm$  15.64 &  22.62 $\pm$  15.27 &   9.47 $\pm$   2.49  \\
          256 nodes &  39.06 $\pm$  26.89 &  24.03 $\pm$  15.30 &   9.49 $\pm$   2.28  \\
          512 nodes &  60.67 $\pm$  49.66 &  25.45 $\pm$  25.18 &   9.56 $\pm$   2.62  \\
         1024 nodes &  87.62 $\pm$  90.67 &  29.31 $\pm$  35.55 &   9.67 $\pm$   2.37  \\
%         2048 nodes &  98.61 $\pm$ 136.20 &  31.39 $\pm$  60.58 &  \:12.18 $\pm$   6.32
%      \Rstrut  \\ \hline
      \thickhline
  \end{tabular} }
\caption{Duration of violations ($Med \pm \sigma$)}
\label{table:detailed_violation_time}
\end{table}
%
% \begin{table}[ht]
% \centering
%     {\scriptsize \begin{tabular}{|P{27mm}@{\:}||@{\:}c@{\:}|@{\:}c@{\:}|@{\:}c@{\:}|}
%       \thickhline
%       \textbf{Infrastructure Size}
%         & \multicolumn{3}{c@{\:}|}{\textbf{Algorithm}}
%           \Tstrut \\
%          \hfill  & ~Centralized~ & ~Hierarchical~ & Distributed \Bstrut \\
%       \thickhline
%         128 nodes   &  3.76 $\pm$  7.43 &  2.52 $\pm$  4.63 &   0.29 $\pm$ 0.03 \\
%         256 nodes   &  7.97 $\pm$ 15.03 &  2.65 $\pm$  4.69 &   0.25 $\pm$ 0.02 \\
%         512 nodes   & 15.71 $\pm$ 29.14 &  2.83 $\pm$  4.98 &   0.21 $\pm$ 0.01 \\
%         1024 nodes  & 26.41 $\pm$ 50.35 &  2.69 $\pm$  4.92 & \:0.14 $\pm$ 0.01
%       \Rstrut  \\ \hline
%       \thickhline
%   \end{tabular} }
% \caption{Duration of computations ($Med \pm \sigma$)}
% \label{table:detailed_computation_time}
% \end{table}
%
\begin{table}[t]
\centering
    {\scriptsize \begin{tabular}{|P{27mm}@{\:}||@{\:}c@{\:}|@{\:}c@{\:}|@{\:}c@{\:}|}
      \thickhline
      \textbf{Infrastructure Size}
        & \multicolumn{3}{c@{\:}|}{\textbf{Algorithm}}
          \Tstrut \\
         \hfill  & ~Centralized~ & ~Hierarchical~ & Distributed \Bstrut \\
      \thickhline
        128 nodes &   5.01 $\pm$   8.38 &   2.70 $\pm$   4.86 &   0.03 $\pm$   0.02   \\
        256 nodes &  10.08 $\pm$  16.96 &   3.32 $\pm$   5.28 &   0.02 $\pm$   0.02   \\
        512 nodes &  16.41 $\pm$  29.32 &   2.86 $\pm$   4.92 &   0.01 $\pm$   0.01   \\
       1024 nodes &  27.60 $\pm$  52.99 &   3.35 $\pm$   5.28 &   0.01 $\pm$   0.02   \\
%       2048 nodes &  90.10 $\pm$ 122.42 &   3.04 $\pm$   5.27 &   \:0.01 $\pm$   0.01
%      \Rstrut  \\ \hline
      \thickhline
  \end{tabular} }
\caption{Duration of computations ($Med \pm \sigma$)}
\label{table:detailed_computation_time}
\end{table}
%
% \begin{table}[ht]
% \centering
%     {\scriptsize \begin{tabular}{|P{27mm}@{\:}||@{\:}c@{\:}|@{\:}c@{\:}|@{\:}c@{\:}|}
%       \thickhline
%       \textbf{Infrastructure Size}
%         & \multicolumn{3}{c@{\:}|}{\textbf{Algorithm}}
%           \Tstrut \\
%          \hfill  & ~Centralized~ & ~Hierarchical~ & Distributed \Bstrut \\
%       \thickhline
%         128 nodes   & 10.34 $\pm$  1.70 &  10.02 $\pm$  0.14 &   10.01 $\pm$ 0.11 \\
%         256 nodes   & 10.26 $\pm$  1.45 &  10.11 $\pm$  0.83 &   10.01 $\pm$ 0.08 \\
%         512 nodes   & 11.11 $\pm$  3.23 &  10.28 $\pm$  1.50 &   10.08 $\pm$ 0.82 \\
%         1024 nodes  & 18.90 $\pm$  7.57 &  10.30 $\pm$  1.60 & \:10.04 $\pm$ 0.63
%       \Rstrut  \\ \hline
%       \thickhline
%   \end{tabular} }
% \caption{Duration of reconfigurations ($Med \pm \sigma$).}
% \label{tab:detailed_reconf_time}
% \end{table}
%
\begin{table}[ht]
\centering
    {\scriptsize \begin{tabular}{|P{27mm}@{\:}||@{\:}c@{\:}|@{\:}c@{\:}|@{\:}c@{\:}|}
      \thickhline
      \textbf{Infrastructure Size}
        & \multicolumn{3}{c@{\:}|}{\textbf{Algorithm}}
          \Tstrut \\
         \hfill  & ~Centralized~ & ~Hierarchical~ & Distributed \Bstrut \\
      \thickhline
        128 nodes &  10.44 $\pm$   1.95 &  10.02 $\pm$   0.14 &  10.01 $\pm$   0.10  \\
        256 nodes &  11.10 $\pm$   2.95 &  10.16 $\pm$   1.22 &  10.00 $\pm$   0.00  \\
        512 nodes &  12.22 $\pm$   5.26 &  10.15 $\pm$   1.19 &  10.01 $\pm$   0.12  \\
       1024 nodes &  20.64 $\pm$   9.87 &  10.22 $\pm$   1.33 &  10.03 $\pm$   0.41  \\
%       2048 nodes &  32.33 $\pm$  16.58 &  10.53 $\pm$   1.85 &  \:13.40 $\pm$   6.06
%      \Rstrut  \\ \hline
      \thickhline
  \end{tabular} }
\caption{Duration of reconfigurations ($Med \pm \sigma$).}
\label{tab:detailed_reconf_time}
\end{table}

% \begin{table}[ht]
% \centering
%     {\scriptsize \begin{tabular}{|P{27mm}@{\:}||@{\:}c@{\:}|@{\:}c@{\:}|@{\:}c@{\:}|}
%       \thickhline
%       \textbf{Infrastructure Size}
%         & \multicolumn{3}{c@{\:}|}{\textbf{Algorithm}}
%           \Tstrut \\
%          \hfill &  ~Centralized~ & ~Hierarchical~ & ~Distributed  \Bstrut \\
%       \thickhline

%         128 nodes &   9.64 $\pm$   0.93 &   9.64 $\pm$   1.03 &   9.65 $\pm$   0.84  \\
%         256 nodes &  10.01 $\pm$   1.99 &   9.61 $\pm$   0.73 &   9.63 $\pm$   0.82  \\
%         512 nodes &  11.05 $\pm$   2.67 &   9.66 $\pm$   0.90 &   9.65 $\pm$   0.79  \\
%        1024 nodes &  23.65 $\pm$   6.65 &   9.92 $\pm$   1.74 &   9.69 $\pm$   0.85

%       \Rstrut  \\ \hline
%       \thickhline
%   \end{tabular} }
% \caption{Duration of migrations ($Med \pm \sigma$).}
% \label{table:detailed_migrations}
% \end{table}

% \begin{table}[ht]
% \centering
%     {\scriptsize \begin{tabular}{|P{27mm}@{\:}||@{\:}c@{\:}|@{\:}c@{\:}|@{\:}c@{\:}|}
%       \thickhline
%       \textbf{Infrastructure Size}
%         & \multicolumn{3}{c@{\:}|}{\textbf{Algorithm}}
%           \Tstrut \\
%          \hfill &  ~Centralized~ & ~Hierarchical~ & ~Distributed \Bstrut \\
%       \thickhline

%         128 nodes & 232   & 334    & 271   \\
%         256 nodes & 307   & 708    & 533   \\
%         512 nodes & 523   & 1578   & 1254  \\
%        1024 nodes & 693   & 1760   & 2632

%       \Rstrut  \\ \hline
%       \thickhline
%   \end{tabular} }
% \caption{Number of migrations.}
% \label{table:detailed_migrations_count}
% \end{table}
%
\begin{table}[t]
\centering
    {\scriptsize \begin{tabular}{|P{27mm}@{\:}||@{\:}c@{\:}|@{\:}c@{\:}|@{\:}c@{\:}|}
      \thickhline
      \textbf{Infrastructure Size}
        & \multicolumn{3}{c@{\:}|}{\textbf{Algorithm}}
          \Tstrut \\
         \hfill &  ~Centralized~ & ~Hierarchical~ & ~Distributed \Bstrut \\
      \thickhline
          128 nodes & 103 &   70  &   93 \\
          256 nodes & 135 &  164  &  194 \\
          512 nodes & 163 &  267  &  344 \\
         1024 nodes & 266 &  651  &  842 \\
%         2048 nodes & 285 & 1174  & 1489
%      \Rstrut  \\ \hline
      \thickhline
  \end{tabular} }
\caption{Number of migrations.}
\label{table:detailed_migrations_count}
\end{table}

Regarding Snooze, whose performance is better than those of Entropy,
we may erroneously conclude that the hierarchical approach is not
competitive compared to the distribued strategy at the first
sight. However, diving into the details, we can see that both the
computation and reconfiguration phases are almost of constant duration
(around 3 seconds and 10 seconds) and not much longer than DVMS's
corresponding phases, especially for the reconfiguration phase, which
is predominant. These results can be easily explained: the centralized
policy adresses the VMPP by considering all nodes at each invovation,
while the hierarchical and the distributed algorithms divide the VMPP
into sub problems, considering smaller numbers of nodes (32~PMs in
Snooze and 4~nodes on average with DVMS). These results raises the
question of the influence of the group size, \ie the ratio of LCs
attached to one GM, on Snooze's performance (a relationship we
investigate in Section \ref{subsec:variant}).

Last but not least, Table \ref{table:detailed_migrations_count}
presents the number of migrations that has been performed overall for
each simulation. While DVMS enables the resolution of violations in a
short time, one of its potential drawbacks is the number of migration
it performs. In addition to adding a significant overhead at the
network level, performing migrations can negatively impact the
performance of the workload running inside the VM during the
relocation phases. Hence, it is usually better to try to find the
right trade-off between the reactivity criterion and the number of VM
migration as investigated by other VM placement strategies
\cite{eyraud:ipdps2013}. Diving into the details, \vmps monitors how
many times each VM has been migrated. This feature is also interesting as
several schedulers, including the ones discussed in the present
article, do not consider whether a VM has been previously migrated or
not.
%For instance, some VMs are migrated up to ten times with DVMS
%while other are never migrated.
Hence, it is possible to migrate the same VM a significant number
of times while never relocating the others. As we have illustrated
here, \vmps enables researchers to monitor such a metric.

\subsubsection{Simulations Scalability}
%\AL[MS,AL]{If we do not have the last results, just remove the
%  subsubsection title and directly put To conclude on this general comparison}
To conclude this general comparison, although the simulations discussed in this article are
limited to 10K VMs, we succeeded to conduct DVMS simulations including
up to 8K~PMs/80K~VMs in a bit less than two days.
Similarly, we performed DVMS simulations for 10K VMs over a simulated period of
3600 and 7200 seconds (\ie. two hours). The duration to get the
results were respectively 7420 and 17198 seconds.
We do not discuss into details these results for the hierarchical
approach a la Snooze because it was not possible to run a
sufficient number of simulations at such scale. The Snooze
protocol being more complex than the DVMS one (heartbeats,
consensus,~\ldots), the time to perform a similar experiment is much
more important (around 7 days for the 80K~VMs experiment). The time-consuming portions of the
code are related to \sg internals such as \texttt{sleep} and
\texttt{send/recv} calls. Hence, we have contacted the \sg core
developers in order to investigate how we can reduce the required time
to perform such advanced simulations.

% \begin{table}
%     {\scriptsize \begin{tabular}{|P{20mm}@{\:}||@{\:}c@{\:}|c@{\:}|}
%       \thickhline
%       \textbf{Simulated Duration}
%         & \multicolumn{1}{c@{\:}|}{\textbf{Simulation duration}}
%           \Tstrut \\
%          \hfill &    \Bstrut \\
%       \thickhline
%         1800 s & 3400 s  \\
%         3600 s & 7420 s  \\
%         7200 s & 17198 s
%       \Rstrut  \\ \hline
%       \thickhline
%   \end{tabular} }
%  \caption{Impact of simulated duration over the simulation duration.}
%  \label{tab:scalability_duration}
% \end{table}


\begin{figure*}[htbp]
\subcapcentertrue
\subfigure[Total Violation Times]{
  \includegraphics[width=.32\textwidth]{figures/groupSizes-violationTime.pdf}
  \label{fig:groupSizesViolationTime}}
\begin{minipage}{.66\textwidth}\centering
  \vspace*{-4.5cm}
\subfigure[No.\ of Failed Reconfigurations]{
    {\scriptsize \begin{tabular}[b]{|r@{\:}||@{\:}r@{\:}|@{\:}r@{\:}|@{\:}r@{\:}|@{\:}r@{\:}|}
      \thickhline
      \textbf{Infra.\ Size~~}
        & \multicolumn{ 4 }{c@{\:}|}{\textbf{No.\ of failed reconfigurations}}
          \Tstrut \\
         \hfill &  ~2 LCs~ & ~4 LCs~ & ~8 LCs~ &  ~32 LCs~  \Bstrut \\
      \thickhline

        128~~~~~~~ &  19~ & 0~ & 0~ & 0~ \\
        256~~~~~~~ &  29~ & 0~ & 0~ & 0~ \\
        512~~~~~~~ &  83~ & 1~ & 0~ & 0~ \\
       1024~~~~~~~ & 173~ & 7~ & 0~ &  0
      \Rstrut  \\ \hline
      \thickhline
  \end{tabular} }
  \label{fig:groupSizesReconfigFail}
  }
\subfigure[Means $\pm$ Std deviations of computation durations.]{
    {\scriptsize \begin{tabular}[b]{|r@{\:}||@{\:}c@{\:}|@{\:}c@{\:}|@{\:}c@{\:}|@{\:}c@{\:}|}
      \thickhline
      \textbf{Infra.\ Size~~}
        & \multicolumn{ 4 }{c@{\:}|}{\textbf{Algorithm}}
          \Tstrut \\
         \hfill &  ~2 LCs~ & ~4 LCs~ & ~8 LCs~ & 32 LCs  \Bstrut \\
      \thickhline

        128~~~~~~~ &   0.16 $\pm$   1.23 &   0.34 $\pm$   1.81 &   0.58 $\pm$   2.40 &   2.53 $\pm$   4.62  \\
        256~~~~~~~ &   0.18 $\pm$   1.31 &   0.42 $\pm$   1.99 &   0.66 $\pm$   2.50 &   2.65 $\pm$   4.69  \\
        512~~~~~~~ &   0.15 $\pm$   1.20 &   0.33 $\pm$   1.78 &   0.67 $\pm$   2.54 &   2.83 $\pm$   4.98  \\
       1024~~~~~~~ &   0.19 $\pm$   1.37 &   0.42 $\pm$   2.02 &   0.89 $\pm$   2.90 &   ~2.69 $\pm$   4.91

      \Rstrut  \\ \hline
      \thickhline
  \end{tabular} }
  \label{fig:groupSizesComputationTime}
  }
\end{minipage}
\caption{Hierarchical placement: influence of varying group sizes}
\label{fig:snoozeGroupSizes}
\end{figure*}

\subsection{Exploring Variants and Possible Improvements}
\label{subsec:variant}
%\MS{Adapt this intro or even the structure depending on other
%  variants.}

In the following, we present several variants of
the placement algorithms introduced in Sec.~\ref{sec:vm-schedulers}
that have been discussed in the literature or come up during the
implementation of their models using \vmps. This section provides
strong evidence that the modification and evaluation of existing
algorithms is much facilitated by our simulation framework.

\subsubsection{Varying group sizes}
\label{sec:groupSizes}

As a first example, we propose to clarify the influence of the group
size observed during the general comparison.
Concretely, we performed additional simulations aiming at
investigating whether a smaller group size for Snooze can lead to similar
performances of DVMS and reciprocally, whether DVMS can increase its
reactivity by integrating more nodes at the first iteration of each
ISP. We highlight that the use of \vmps eased such a study as it has consisted
to simply relaunch the previous simulation with a distinct
assignment for Snooze and to slightly modify the ISP algorithm for DVMS.


Figure \ref{fig:snoozeGroupSizes} presents the simulated values
obtained for the Snooze scenarios with 2,~4,~8 and 32~LCs per GM for four
infrastructure sizes. The overall performance (\ie cumulated violation
time), shown in Fig.~\ref{fig:groupSizesViolationTime}, shows that
2~LCs per GM result in significantly higher violation times. All other
group sizes yield violation times that are relatively close, which
indicates that a small group size does not help much in
resolving violations faster.

The relatively bad performance of the smallest group size can be
explained in terms of the number of failures of the reconfiguration
process, that is, overloading situations that are discovered but
cannot be resolved because the GM managing the overloaded VM(s) did
not dispose of enough resources, see
Table~\ref{fig:groupSizesReconfigFail}. Groups of 2~LCs per GM are
clearly insufficient at our load level (60\% mean, 20\% stddev).
Failed reconfigurations are, however, already very rare in the case of
4~LCs per GM and do not occur at all for 8~and 32~LCs per GM. This is
understandable because the load is statistically evenly distributed
among the LCs and tthe load profile we evaluated only rarely results
in many LCs of a GM to be overloaded. Violations can therefore be
resolved even in the case of a smaller number (4) LCs available for
load distribution.

Conversely, we can see that the duration of the overall
reconfiguration phases decreases strongly along with the group
size. It reaches a value close to the computation times of DVMS for a
group size of 4-LCs per GM, see Fig.~\ref{fig:groupSizesComputationTime}.
We thus cannot minimize computation times and violation times by
reducing the number of LCs because larger group sizes are necessary to
resolve overload situations if the VM load gets higher.  Once again,
this information is valuable as it will help researchers to design new
algorithms favoring the automatic discovery of the optimal subset of
nodes capable to solve violations under for given load profiles.

In contrast, DVMS resolves this trade-off by construction because of its
automatic and dynamic choice of the partition size necessary to handle
an overloaded situation.
However, it can be interesting to directly add more than one node to
form the first group, \aka microcosm in the DVMS terminology,
especially because having only two nodes seems to be not enough as
depicted by the previous Snooze scenarios.


% \begin{table}[ht]
% \centering
%     {\scriptsize \begin{tabular}{|P{27mm}@{\:}||@{\:}c@{\:}|@{\:}c@{\:}||@{\:}c@{\:}|@{\:}c@{\:}|}
%       \thickhline
%       \textbf{Infrastructure Size}
%         & \multicolumn{2}{c@{\:}||@{\:}}{\textbf{No.\ migrations}}
%         & \multicolumn{2}{c@{\:}|}{\textbf{Violation time (s)}}
%           \Tstrut \\
%          \hfill flavour &  ~Original~ & 4 nodes  &  ~Original~ & 4 nodes \Bstrut \\
%       \thickhline

%         128 nodes &   78  & 76  & 914.28  & 884.46  \\
%         256 nodes &   152 & 127 & 1709.23 & 1498.71 \\
%         512 nodes &   308 & 286 & 3513.18 & 3314.72 \\
%        1024 nodes &   702 & 616 & 7967.03 & \:7101.49

%       \Rstrut  \\ \hline
%       \thickhline
%   \end{tabular} }
% \caption{Comparison of two DVMS flavours.}
% \label{tab:dvms_flavours}
% \end{table}

% \begin{table}[ht]
% \centering
%     {\scriptsize \begin{tabular}{|P{20mm}@{\:}||@{\:}c@{\:}|@{\:}c@{\:}||@{\:}c@{\:}|@{\:}c@{\:}||@{\:}c@{\:}|@{\:}c@{\:}|}
%       \thickhline
%       \textbf{Infrastructure Size}
%         & \multicolumn{2}{c@{\:}||@{\:}}{\textbf{No.\ migrations}}
%         & \multicolumn{2}{c@{\:}||@{\:}}{\textbf{No.\ violations}}
%         & \multicolumn{2}{c@{\:}|}{\textbf{Violation time (s)}}
%           \Tstrut \\
%          \hfill flavour &  ~Original~ & 4 nodes  &  ~Original~ & 4 nodes &  ~Original~ & 4 nodes \Bstrut \\
%       \thickhline

%         128 nodes &   78  & 76  &   77 &  79 & 11.57 $\pm$   3.44 & 11.49 $\pm$   3.51 \\
%         256 nodes &   152 & 127 &  131 & 156 & 10.96 $\pm$   3.44 & 11.44 $\pm$   3.62 \\
%         512 nodes &   308 & 286 &  282 & 310 & 11.33 $\pm$   3.56 & 11.75 $\pm$   3.12 \\
%        1024 nodes &   702 & 616 &  620 & 698 & 11.41 $\pm$   3.36 & \:11.45 $\pm$ 3.39

%       \Rstrut  \\ \hline
%       \thickhline
%   \end{tabular} }
% \caption{Comparison of two DVMS flavours.}
% \label{tab:dvms_flavours}
% \end{table}

\begin{table}[ht]
\centering
    {\scriptsize \begin{tabular}{|P{20mm}@{\:}||@{\:}c@{\:}|@{\:}c@{\:}||@{\:}c@{\:}|@{\:}c@{\:}|}
      \thickhline
      \textbf{Infrastructure Size}
        & \multicolumn{2}{c@{\:}||@{\:}}{\textbf{Violation time (s)}}
        & \multicolumn{2}{c@{\:}|}{\textbf{No.\ migrations}}
          \Tstrut \\
         \hfill flavour &  ~Original~ & 4 nodes  &  ~Original~ & 4 nodes \Bstrut \\
      \thickhline

        128 nodes & 9.47 $\pm$   2.49 &   9.39 $\pm$   3.08 &    93 &   106 \\
        256 nodes & 9.49 $\pm$   2.28 &   9.61 $\pm$   2.35 &   194 &   182 \\
        512 nodes & 9.56 $\pm$   2.62 &   9.65 $\pm$   2.62 &   344 &   315 \\
       1024 nodes & 9.67 $\pm$   2.37 & \:9.95 $\pm$   2.41 &   842 & \:802

      \Rstrut  \\ \hline
      \thickhline
  \end{tabular} }
\caption{Comparison of two DVMS flavours.}
\label{tab:dvms_flavours}
\end{table}

%\AL[JP]{Why the original values do not correspond to the values in
%  Table 1}

Table \ref{tab:dvms_flavours} presents the violation time and the
number of migrations for the DVMS vanilla implementation and our
variant that directly integrates four nodes at the first step of the
ISP.
In addition to have a smaller violation time,
it is noticeable that
the number of migrations is slightly smaller in the DVMS variant.
This is due to the quality of reconfiguration plans: as more nodes can
be used to rebalance the VMs workload, its efficiency is improved.


% \subsubsection{Variants of hierarchical scheduling}
% \label{sec:snoozeVariants}

% We now present three non-trivial variants that we have implemented and
% explored: periodic vs.\ reactive scheduling, a variant of the
% assignment algorithm of LCs to GMs, and a variant of the algorithms of
% how GMs and LCs join the system.  \MS{How and where do we provide the
%   scheduling parameters for the evals.?}

\subsubsection{Hierarchical scheduling: periodic vs.\  reactive}

\begin{figure*}[htbp]
\subcapcentertrue
\subfigure[DVMS]{
  \includegraphics[width=.32\linewidth]{figures/experiments/clouds-1024-distributed.pdf}
  \label{fig:violation_clouds_dvms_1024}}
\subfigure[Snooze Periodic]{
  \includegraphics[width=.32\linewidth]{figures/experiments/clouds-1024-hierarchical-periodic-30-gm32.pdf}
  \label{fig:violation_clouds_snooze_1024_periodic}}
\subfigure[Snooze Reactive]{
  \includegraphics[width=.32\linewidth]{figures/experiments/clouds-1024-hierarchical-reactive-gm32.pdf}
  \label{fig:violation_clouds_snooze_1024_reactive}}
\caption{Details of violations duration occuring during simulation (10240 VMS).}
\label{fig:violation_clouds}
\end{figure*}

Our vanilla implementation of Snooze~\cite{feller:ccgrid12} schedules VMs in a periodic fashion as
introduced before.  Using \vmps, we have also developed an
alternative, reactive, strategy to scheduling: as soon as resource
conflicts occur, LCs avert their GMs of them; the GMs then immediately
initiate scheduling. Implementing this reactive scheme can be done
using our framework in two manners. First, by implementing additional
asynchronous communication of the necessary state updates as a real
implementation would proceed. Second, in a more lightweight manner
through direct accesses by the GMs to the states of their respective
LCs. In order to ensure that this lightweight implementation mimics a
real implementation closely, delays induced by communication in the
``real'' implementation are accounted for explicitly (congestion
issues are not relevant in this case because notification of a
resource conflict implies little communication and conflict resolution
blocks the GM and its LCs anyway). We have implemented this
lightweight variant of reactive scheduling including an explicit model
of communication delays. Using the abstractions provided by \vmps,
%in
%particular harnessing its extended notion of hosts that represent the
%VMs managed by the LCs of a VM,
reactive scheduling has been
implemented by adding or modifying just 4~lines of code of the variant
with periodic scheduling.



\begin{table}[htbp]
    {\scriptsize \begin{tabular}{|P{20mm}@{\:}||@{\:}c@{\:}|@{\:}c@{\:}||@{\:}c@{\:}|@{\:}c@{\:}|}
      \thickhline
      \textbf{Infrastructure Size}
        & \multicolumn{2}{c@{\:}||@{\:}}{\textbf{Violation time (s)}}
        & \multicolumn{2}{c@{\:}|}{\textbf{No.\ migrations}}
          \Tstrut \\
         \hfill &  ~period 30~ & reactive  \Bstrut
         \hfill &  ~period 30~ & reactive  \Bstrut \\
      \thickhline

        128 nodes & 20.92 $\pm$  13.02 &  10.24 $\pm$   4.48 &  62  & 107   \\
        256 nodes & 23.33 $\pm$  14.47 &  10.02 $\pm$   3.32 &  124 & 201   \\
        512 nodes & 23.57 $\pm$  20.19 &  10.62 $\pm$   7.21 &  269 & \:421

      \Rstrut  \\ \hline
      \thickhline
  \end{tabular} }
 \caption{Snooze periodic \vs Snooze reactive.}
 \label{tab:reactivePeriodic}
\end{table}


% \AL[MS]{Update the table: it may make
% sense to put all values to make the read and the understanding easier
% Periodic 32 GMs, Reactive 32 GMs and finally DVMS}

We have simulated reactive scheduling and a periodic algorithm for
configurations ranging from 128 to 512~LCs. In each case the Entropy
scheduler has been applied to groups of 8~LCs per GM, the variant of
hierarchical scheduling that performs most efficiently and closely
matches the efficiency of DVMS, see Sec.~\ref{sec:groupSizes}.
% The periodic strategy has computed and applied reconfiguration plans
% every 30 secs.
These simulations have yielded the results shown in
Table~\ref{tab:reactivePeriodic}. They clearly show that, while a
reactive strategy entails a much higher number of migrations (because
the periodic one aggregates overload situations and misses some of
them), reactive scheduling results in a significantly lower total
violation time.

% \MS[AL]{Figure \ref{fig:violation_clouds} should be discussed after
%   the reactive discussion, the figure illustrates the pros of the reactivity.}
% \paragraph{Assignment of LCs to GMs}

% LCs are assigned to GMs by the GL as part of the LC join protocol. In
% Snooze's native implementation LCs are assigned in a round-robin
% fashion to the known GMs. If GMs join (and leave) the system at the
% same time as LCs, a round-robin (RR) strategy at join time, however,
% does not ensure an even distribution. This may happen, for instance at
% startup time of the system, when new GMs and LCs enter the system, or
% in case of failures, which trigger GM and LC joins. In order to
% evaluate the corresponding imbalance and its consequences we have
% implemented the LC assignment protocol in a modular fashion and
% applied it to different highly-dynamic settings in which GMs and LCs
% enter the system at the same time. Furthermore, we have implemented a
% best-fit (BF) strategy that assigns LCs to the GMs with minimal load
% or, if several GMs with minimal load exist, to the GMs with the
% smallest number of assigned LCs.


% % \subsubsection{LC assignment in Snooze-like placement alg.}
% % \label{sec:snoozeVariantsEval}

% % \begin{figure}[ht]
% % \begin{center}
% %     \includegraphics[width=.95\linewidth]{figures/violationTime-snooze-RR-BF.pdf}
% %     \caption{Cumulated violation time for BF (lower line) and RR
% %       (upper line) variants}
% % \end{center}
% % \label{fig:snoozeBFRRViolation}
% \end{figure}

% \begin{table*}[ht]
% \begin{center}
% %    \begin{tabular}{|c!{\vrule width 3pt}c|c|c!{\vrule width 2pt}c|c|c!{\vrule width 3pt}c|c!{\vrule width 2pt}c|c|}
%     \begin{tabular}{|P{30mm}|||c|c||c|c|||M{30mm}|M{30mm}|}
%         \thickhline
%         \multirow{3}*{\textbf{Strategy}}
%           & \multicolumn{4}{c|||}{\textbf{\#LCs/\#GMs}}
%           & \multicolumn{2}{c|}{\textbf{Total violation time (s)}}
%           \Tstrut \\
%           & \multicolumn{2}{c||}{128 LCs, 12 GMs}
%             & \multicolumn{2}{c|||}{256 LCs, 25 GMs}
%           & \multirow{2}*{128 LCs, 12 GMs} & \multirow{2}*{256 LCs, 25 GMs}
%           \Bstrut \\
%           & range & stdev & range & stdev & &  \Bstrut \\
%         \thickhline
%         Best-Fit & 0--30 & 10.53 & 0--18 & 6.62  & 395 & 1005 \Rstrut \\
%         Round-Robin & 0--49 & 15.7  & 0--35 & 12.22 & 630 & 1265
%         \Rstrut \\ \hline
%         \thickhline
%     \end{tabular}
% \end{center}
%     \caption{LCs to GM assignment and cumulated violation times for RR
%       and BF strategies}
%     \label{tbl:assignmentResults}
% \end{table*}


% We have evaluated the two LC assignment strategies using \vmps on
% configurations of~128 and~256 nodes. In order to clearly expose the
% corresponding differences this evaluation has been performed by
% ``stressing'' the two strategies by simultaneously
% starting all LCs (128/256) and all GMs (12/25) and then simulating the
% resulting configuration over a one hour period.  These experiments
% have yielded the following results, cf.\ Table~\ref{tbl:assignmentResults}:
% \begin{itemize}
%   \item BF yields more homogeneous assignments of LCs to GMs: the
%     ranges of the numbers of LCs assigned to a GM and their standard
%     deviations are significantly smaller.\footnote{Here, some GMs may
%       be assigned 0 LCs if some GMs join the system after all LCs have
%       been assigned.}
%   \item The cumulated time spent resolving violations is, for both
%     configurations, significantly smaller for BF than for RR.
% \end{itemize}
% From these results, we can clearly infer that BF is significantly
% better than RR for the two tested kinds of configuration. Furthermore,
% we conjecture that BF should perform at least as good as RR for all
% configurations (the proof is left to future work).
% % \MS[JP]{I need the exact figures for the violation times
% %   in Tbl.~\ref{tbl:assignmenResults}}

% \paragraph{Variants of the join algorithms}

% The join algorithms, see Sec.~\ref{sec:snoozeAlgs}, are crucial to
% Snooze for two main reasons: (i) they have to be efficient because
% they can easily form a bottleneck if large numbers of LCs (GMs) have
% to be registered at a GM (LC); (ii) they are multi-phase protocols
% whose correctness especially in the presence of faults is difficult to
% ensure.

% In order to investigate the corresponding trade-offs, we have used our
% framework to implement join algorithms that may be interrupted at any
% time, repeat the the on-going phase a number of times before
% reinitiating, if necessary, the entire protocol. Furthermore, the join
% protocol is parameterized, \eg, in the number of threads used to
% handle registration requests.

% Finally, our framework has enabled us to test another aspect of
% Snooze's join algorithm as presented by
% Feller~\etal.~\cite{feller:ccgrid12}, a strategy we call the GM rejoin
% strategy (GRJ): all GMs and the LCs assigned to them should rejoin if
% a new GM enters the system. While GRJ supports a form of load
% balancing (because all LCs are reassigned to the new set of GMs), our
% simulation has shown that this strategy significantly increases the
% time necessary for registering GMs and LCs compared to a simpler
% strategy that does not modify existing GMs in case a new GM enters the
% system. This handicap is particularly pronounced if joins of GMs may
% be interrupted due to faults. Concretely, experiments involving 20 GMs
% and 200 LCs have shown that this strategy often multiplies the time
% necessary to join all 220 components by 10 or more compared to the
% simple join strategy. While the qualitative result that the more
% complex strategy presented in the paper results in a more
% time-consuming join process is not very surprising, the extent of the
% resulting degradation was surprising.


\subsubsection{Analysis of Fault Tolerance Property}
\label{subsubsec:node-crashes}
We have performed a series of analysis of fault tolerance
properties. For space reasons, we only detail below some corresponding
results for the hierachical placement strategy. It turns out that
Snooze's strategy based on heartbeats enables the reconstruction of
the hierarchy in a relative short time and thus crashes on service
nodes only moderately impact the resolution of violations even in the
case of high failure rates.
% (in our case less than 10 seconds is mandatory to reorganize the
% Snooze topology with a 6 seconds heartbeat mechanism).

As to Entropy, although the loss of the service node can be critical,
its failure probability is so small that the single point of failure
issue can be easily solved by a fail-over approach.  Regarding DVMS,
the crash of one node does not have any impact on the resolution as
the composition of the microcosms is reevaluated immediately.

%\Paragraph{Fault tolerance analysis of hierarchical scheduling.}


Regarding the hierarchical scheduling a la Snooze, we have also
harnessed VMPlaceS in order to analyze and present to the best of our
knowledge, a first analysis of such property.
We have, in particular, compared standard executions (no faults, 32
LCs per GM, periodic calls every 30s to the centralized solver) to
executions that have been subject to faults. In order be able to
clearly evaluate the influence of faults, we have decided to analyze
faulty executions under heavy stress: for a simulated time frame of
1800s, the following numbers of GM faults have been simulated: 10~GM
faults for 256~LCs/8~GMs, 20~GM faults for 512~LCs/16~GMs, and 40~GM
faults for 1024~LCs/32~GMs, that is, failure rates much higher than in
real datacenters.


% \begin{figure}
%   \parbox{.47\linewidth}{\centering
%   \includegraphics[width=.95\linewidth]{figures/snoozeFault-successPsize.pdf}

%   (a) GM size (\#LCs)
%   }
%   \
%   \parbox{.47\linewidth}{\centering
%   \includegraphics[width=.95\linewidth]{figures/snoozeFault-violationTime.pdf}

%   b) Violation times
%   }
%   \caption{Behavior in the presence of faults}
%   \label{fig:snoozeFaults}
% \end{figure}

\begin{figure}[htbp]
\subcapcentertrue
\subfigure[GM Size (\#LCs)]{\includegraphics[width=4cm]{./figures/snoozeFault-successPsize.pdf}\label{fig:snooze-fault-psize}}
\subfigure[Cumulated Violation Time]{\includegraphics[width=4cm]{./figures/snoozeFault-violationTime}\label{fig:snooze-fault-viol}}
\caption{Behavior in the presence of faults}
\label{fig:snoozeFaults}
\end{figure}


Figure~\ref{fig:snoozeFaults} shows two principal metrics for the
standard (non-faulty) and faulty executions: the average GM sizes (in
terms of LCs) during reconfigurations (left) and the violation times
(right).

The results show that the average GM sizes are moderately affected by
faults: 32 LCs in the case of non-faulty executions, 39--46 LCs in the
case of the faulty executions. However, the violation times are rather
strongly affected: 3453--18998s for the non-faulty executions,
7284--28636s in the presence of faults. This is very probably the case
because the faults disturb on-going and future resolution actions
triggered that are triggered by violations.



\section{Related Work}
\label{sec:related}
%\AL[AL]{Add few lines related to Google scheduling simulator - https://code.google.com/p/cluster-scheduler-simulator/}

Jobs/tasks scheduling in distributed
system is an old challenge and thus several simulators have been
proposed to investigate pros and cons of new strategies for several years. As a recent
example, Google has released the simulator~\footnote{\href{https://github.com/google/cluster-scheduler-simulator}{https://github.com/google/cluster-scheduler-simulator}} it used for the Omega
framework \cite{schwarzkopf:2013}. However, jobs/tasks scheduling
simulators do not consider the notion of VM and its associated
capabilities (suspend/resume, migrate) and thus are not appropriate
to investigate CC production platforms.

Simulator toolkits that have bee proposed to address CC
concerns~\cite{cloudsim, CC13, DGSIM,  greencloud,
  icancloud} can be classified into two categories: The first
one corresponds to ad-hoc simulators that have been developped to
address a particular concern. For instance, CReST~\cite{CC13} is a
discrete event simulation toolkit built to evaluate Cloud provisioning
algorithms. If ad-hoc simulators enable to provide some trends
regarding the bevahiours of the system, they do consider the
implication of the different layers, which can lead to non
representative results at the end. Moreover, such ad-hoc solutions are
developed for one shot and thus, they are not available for the
scientific community. The second category \cite{cloudsim, greencloud,
  icancloud} corresponds to more generic cloud simulator toolkits (\ie
they have been designed to adress a majority of CC
challenges). However, they have focused mainly on the API and not on
the model of the different mechanisms of CC systems.

For instance, CloudSim~\cite{cloudsim}, which has been widely used to
validate algorithms and applications in different scientific
publications, is based on a relatively top-down viewpoint of cloud
environments.  That is, there is no papers that properly validate the
different models it relies on: a migration time is calculated by
dividing a VM memory size by a network bandwidth.
%Such a model cannot correctly simulate many real
%environments where workloads perform substantial memory writes.
 In addition to having inaccuracy weaknesses at the low level, available cloud
simulator toolkits over simplified the model for the virtualization
technologies, leading also to non representation results at the
end. As highlighted several times throughout this document, we chose to
build \vmps on top of \sg in order to benefit fromt its accuracy of
its models related to virtualization abstractions~\cite{Hirofuchi:2013:ALM:2568486.2568524}.


 is a old other simulators to investigate
scheduling challenges have been proposed

\section{Conclusion}
\label{sec:conclusion}
In this paper we have presented
\vmps, a framework providing programming support for the definition of
VM placement algorithms, execution support for their simulation at
large scales, as well as new means for their trace-based analysis.
\vmps enables, in particular, the investigation of placement
algorithms in the context of numerous and diverse real-world
scenarios. We have validated its accuracy of returned results by
comparing simulated and \textit{in-vivo} executions of the Entropy
strategy on top of 32~PMS and 192~VMs. We have illustrated the
relevancce of \vmps by evaluating and comparing algorithms
representative for three different classes of virtualization
environments: centralized, hierarchical and fully distributed
placement algorithms. We have also shown how \vmps facilitates the
implementation and evaluation of variants of placement algorithms. The
corresponding experiments have provided the first systematic results
comparing these algorithms in environments including up to one
thousand of nodes and ten thousands of VMs.

The current version of \vmps is available on a public git
repository\footnote{\url{http://beyondtheclouds.github.io/VMPlaceS/}}.
We are in touch with the \sg core developers in order to improve ou
code with the ultimate objective of adressing infrastructures up to
100K PMs and 1 Millions VMs. As future work, it would be valuable to add
additional dimensions in order to simulate other workload variations
stemming from network and HDD I/O changes. Finally,
we plan to provide a dedicated API to be able to
provision and remove VMs during the execution of a simulation.
% conference papers do not normally have an appendix

\section{Acknowledgment}

This work is supported by the French ANR project SONGS (11-INFRA-13).
Experiments have been performed using the Grid'5000 experimental
testbed, being developed under the INRIA ALADDIN development action
with support from CNRS, RENATER and several Universities as well as
other funding bodies (see https://www.grid5000.fr).


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% End:
