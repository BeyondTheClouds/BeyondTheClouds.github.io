\section{Context and Motivations}
\label{sec:intro}
The success of Cloud Computing has driven the advent of Utility Computing.
However Cloud Computing is a victim of its own success: In order to answer the
ever increasing demand for computing resources, Cloud Computing providers must
build data centers (DCs) of ever-increasing size. Besides facing the well-known
issues of large-scale platforms management, large-scale DCs have to deal with
energy considerations that limit the number of physical resources that one
location can host.
%
However, instead of investigating other solutions that can tackle both
concerns, the current trend consists in deploying  DCs in few
strategic locations that deliver energy advantages.  For example, Western North
Carolina, USA, is an attractive area due to its abundant capacity of coal
and nuclear power following the departure of the region's textile and furniture
manufacturing \cite{}.  More recently, several proposal suggest building next
generation DCs close to the polar circle  in order to leverage free cooling
techniques, considering that cooling is accounting for a big part of the
consumed electricity \cite{}. 


\subsection{Inherent limitation of large-scale DCs}

Although building large scale DCs  enables to cope with the actual demand while
continuing to operate UC resources through centralized software system, it is
far from delivering sustainable and efficient UC infrastructures.  In addition
to requiring the construction and the deployment of a complete network
infrastructure to reach each DC, it exacerbates the inherent limitations of the
Cloud Computing model:

\begin{itemize}
\item The externalization of private applications/data often faces legal issues
that restrain companies from outsourcing them on external infrastructures,
especially when located in other countries. 
\item The overhead implied by the unavoidable use of the Internet to reach
distant platforms is wasteful and costly in several situations: Deploying a
broadcasting service of local events or an online service to order pizza at the
edge of the polar circle leads to important overheads since it can be assumed
that a vast majority of the users are located in the neighborhood of the
event/the pizzeria.  
\item The connectivity to the application/data cannot be ensured by centralized
dedicated centers, especially if they are located in a similar geographical
zone. The only way to ensure disaster recovery is to leverage distinct
sites.\footnote{“Amazon outages – lessons learned”,
\href{http://gigaom.com/cloud/amazon-outages-lessons-learned/}{http://gigaom.com/cloud/amazon-outages-lessons-learned/}
(valid on Oct 2013, the 15\textsuperscript{th}).} 
\end{itemize}

Although hybrid or federated Cloud solutions~\cite{armbrust:2010} that aim at extending
the resources available on one Cloud  with another one
can partially tackle the two former points, the latter requires a disruptive change in
the way UC resources are managed. Deploying a local events broadcasting service or an
online service to order pizza at the edge of the polar circle leads to an important overhead
in terms of energy footprint, network exchanges as well as latency since it can be assumed
that a vast majority of the users are located in the neighborhood of the event/the
pizzeria.  According to some projections of a recent IEEE report
\cite{ieeenetreport:2012}, the network traffic continues to double roughly each
year. Bringing the IT services closer to the end-users is becoming crucial to limit
the energy impact of these exchanges and to save the bandwidth of some links. Similarly,
this notion of locality is also critical for the adoption of the UC model by applications
that need to deal with a large amount of data as getting them in and out actual UC
infrastructures may significantly impact the global performance~\cite{Fos11}. 

The concept of micro data centers at the edge of the backbone
\cite{greenberg:sigcomm09} may be seen as the complementary solution to hybrid
platforms in order to reduce the overhead of network exchanges.  However, the
number of such micro data centers will remain limited and the question of how
federating a large number of such facilities is still not solved.  

At the same time, people are (and will be more and more) surrounded by
computing resources, especially the ones in charge of interconnecting all IT
equipments.  Even though these small and medium-size facilities include
resources that are sometimes barely used, they can hardly be removed (\textit{e.g.} routers). As
a consequence, several initiatives started investigating how they could be
better leveraged to support the requirements and constraints of current IT
usages.  The concept of \emph{data furnaces} \cite{liu:hotcloud11} is one of
the promising idea that seeks to mitigate the cost of operating
network/computing resources by using them as a source of heat inside public
buildings such as hospitals or universities. 

%Our vision is that both network as well as UC resources should be operated in a
%federated fashion. This is the only way to deliver a
%sustainable as well as an efficient UC model.

\subsection{Oversized Network Backbone.}
Figure \ref{fig:renater} is a snapshot of the network weather
map of RENATER\footnote{\href{http://www.renater.fr}{http://www.renater.fr}}, the network backbone dedicated to universities and research
institutions in France. It reveals several important points: 
\begin{itemize} 
\item It clearly shows that most of the resources are barely used (only two links are used between 45\% and 55\%, a few between 25\% and 40\% and the majority below the threshold of 25\%). 
\item The backbone has been deployed and is renewed according to demand: the density of
points of presence (PoP) of the network as well as the bandwidth of each link are more important on the edge of large cities such as Paris, Lyon or
Marseille. 
\item The backbone has been deployed in order to face disconnections, \textit{i.e.} 95\% of the PoPs can be reached by at least two distinct routes.
\end{itemize}


\begin{figure}[b]
\includegraphics[width=12cm]{./FIGS/renater.png}
\vspace*{-.3cm}
\label{fig:renater}
\caption{The RENATER Weather Map on May 2013, the 27th, around 4PM.}
\centering {\small Available in real-time
at: \href{http://www.renater.fr/raccourci}{http://www.renater.fr/raccourci}}
\vspace*{-.3cm}
\end{figure}

\subsection{Locality Based Utility Computing}

\ftodo[CT$\rightarrow$AL]{Change title --- something with ``LUC'' / ``our proposal''}

This chapter aims at introducing a new generation of UC platforms that can be
seen somehow as an extension of the concept of micro DCs: 
Instead of building and deploying dedicated facilities, we claim that next UC
infrastructures should be tightly coupled with any facilities available through
the Internet, starting from the core routers of the backbone, the different
network access points and any small and medium-size computing infrastructures
that may be provisioned by public and private institutions. 
 Although it involves radical changes in the way
physical and virtual resources are managed, locating and operating computing
power and data on
facilities close to the end-users is the only way to deliver highly efficient
and sustainable UC services. 

From the physical point of view, network backbones such as the RENATER one provide
appropriate infrastructures, that is, reliable and efficient enough to operate UC
resources spread across the different PoPs. Ideally, UC resources would be able to
directly
take advantage of computation cycles available on network servers, \textit{i.e.} the one
in charge of routing packets. However, leveraging network resources to make external
computations may lead to important security concerns. Hence, we propose to extend each
network hub with a number of servers dedicated to host VMs. As we can expect that the
distribution between network traffics and UC demands would be proportional, larger network
hubs will be completed by more UC resources than the smaller ones. Moreover by deploying
UC services on relevant PoPs, a LUC infrastructure will be able to natively confine
network exchanges to a minimal scope, minimizing both the energy footprint of the network
and the impact on latency.

\ftodo[CT$\rightarrow$AL]{LUC is first used here, without being defined.}

From the software point of view, the main challenge is to design a complete distributed
system in charge of turning a complex and diverse network of resources into a collection
of abstracted computing facilities that is both reliable and easy to operate.

\begin{svgraybox}
By designing an advanced system that offers the possibility to operate a large
number of UC resources spread throughout distinct sites \ie, a \emph{LUC
  Operating System}, in a unified manner,
ISPs as well as academic and private institutions in
charge of operating a network backbone will be able to build an extreme-scale
LUC infrastructure with a limited additional cost. Instead of redeploying a
complete installation, they will be able to leverage IT resources and
specific devices such as computer room air conditioning units, inverters or
redundant power supplies already present on each hub of their
backbone. 
\end{svgraybox}


\medskip
%This chapter  describes  how such a new
%generation of highly efficient and sustainable UC can emerge through an integrated
%system, \ie the \emph{LUC Operating System}, leveraging advanced and P2P system mechanisms.

In addition to consider \emph{Locality} as a primary concern, the novelty of the LUC OS
proposal is to consider VM as the basic object it manipulates.  Unlike existing
research on distributed operating systems that have been designed on the process concept, a LUC OS will manipulate VMs throughout a federation of widely distributed
physical machines. Virtualization technologies abstract out hardware heterogeneity, and allow
transparent deployment, preemption, and migration of virtualized
environments (VEs), \ie a set of interconnected VMs.
By dramatically increasing the flexibility of resource management, virtualization 
allows to leverage state-of-the-art results from other distributed
systems areas such as autonomous and decentralized systems.  
Our goal is to build a system that allows end-users to launch VEs over a
distributed infrastructure as simply as they launch processes on a
local machine, \ie  without the burden of dealing with resources
availability or location.

%\paragraph{Chapter Outline.} 
Section \ref{sec:challenges} describes the key objectives of a LUC OS and the associated challenges. 
Section \ref{sec:background} explains why our vision differs from actual and previous UC solutions. In
Section \ref{sec:archi}, we present how such a unified system may be designed
by delivering the premises of the \discovery system, an agent-based system
enabling distributed and cooperative management of virtual environments over a
large-scale distributed infrastructure.
Future work as well as opportunities  are addressed in Section \ref{sec:future}. Finally Section~\ref{sec:conclusion} concludes this chapter. 
